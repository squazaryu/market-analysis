#!/usr/bin/env python3
"""
–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –º–∞–ø–ø–µ—Ä –ë–ü–ò–§ –Ω–∞ investfunds.ru
–ù–∞—Ö–æ–¥–∏—Ç ID —Ñ–æ–Ω–¥–æ–≤ –ø–æ –Ω–∞–∑–≤–∞–Ω–∏—é, ISIN –∏ –¥—Ä—É–≥–∏–º –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º
"""

import pandas as pd
import requests
from bs4 import BeautifulSoup
import re
import time
import json
from typing import Dict, List, Optional, Tuple
import logging
from pathlib import Path
from urllib.parse import quote
import difflib
from dataclasses import dataclass

@dataclass
class FundMatch:
    """–†–µ–∑—É–ª—å—Ç–∞—Ç –ø–æ–∏—Å–∫–∞ —Ñ–æ–Ω–¥–∞"""
    fund_id: Optional[int]
    confidence: float  # 0-1, —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –≤ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–∏
    name_match: str
    url: str
    reason: str

class FundMapper:
    """–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –º–∞–ø–ø–µ—Ä –ë–ü–ò–§ –Ω–∞ investfunds.ru"""
    
    def __init__(self, output_file: str = "fund_mapping.json"):
        self.base_url = "https://investfunds.ru"
        self.output_file = Path(output_file)
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
        })
        
        self.logger = self._setup_logger()
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –º–∞–ø–ø–∏–Ω–≥
        self.mapping = self._load_existing_mapping()
        
        # –ö–µ—à –¥–ª—è –ø–æ–∏—Å–∫–∞
        self.search_cache = {}
        
    def _setup_logger(self) -> logging.Logger:
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–≥–µ—Ä–∞"""
        logger = logging.getLogger('FundMapper')
        logger.setLevel(logging.INFO)
        
        if not logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
            handler.setFormatter(formatter)
            logger.addHandler(handler)
            
        return logger
    
    def _load_existing_mapping(self) -> Dict[str, int]:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –º–∞–ø–ø–∏–Ω–≥"""
        if self.output_file.exists():
            try:
                with open(self.output_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    return data.get('mapping', {})
            except Exception as e:
                self.logger.warning(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–∞–ø–ø–∏–Ω–≥–∞: {e}")
        
        # –ò–∑–≤–µ—Å—Ç–Ω—ã–µ –º–∞–ø–ø–∏–Ω–≥–∏
        return {
            'LQDT': 5973,
        }
    
    def save_mapping(self):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –º–∞–ø–ø–∏–Ω–≥ –≤ —Ñ–∞–π–ª"""
        data = {
            'mapping': self.mapping,
            'last_updated': time.strftime('%Y-%m-%d %H:%M:%S'),
            'total_funds': len(self.mapping)
        }
        
        try:
            with open(self.output_file, 'w', encoding='utf-8') as f:
                json.dump(data, f, indent=2, ensure_ascii=False)
            self.logger.info(f"–ú–∞–ø–ø–∏–Ω–≥ —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {len(self.mapping)} —Ñ–æ–Ω–¥–æ–≤")
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–∞–ø–ø–∏–Ω–≥–∞: {e}")
    
    def search_fund_by_name(self, fund_name: str) -> List[FundMatch]:
        """–ò—â–µ—Ç —Ñ–æ–Ω–¥ –ø–æ –Ω–∞–∑–≤–∞–Ω–∏—é"""
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–µ—à
        cache_key = fund_name.lower().strip()
        if cache_key in self.search_cache:
            return self.search_cache[cache_key]
        
        matches = []
        
        try:
            # –û—á–∏—â–∞–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ –¥–ª—è –ø–æ–∏—Å–∫–∞
            clean_name = self._clean_fund_name(fund_name)
            search_query = quote(clean_name)
            
            # –ü–æ–∏—Å–∫ —á–µ—Ä–µ–∑ —Å—Ç—Ä–∞–Ω–∏—Ü—É —Å–æ —Å–ø–∏—Å–∫–æ–º —Ñ–æ–Ω–¥–æ–≤
            funds_url = f"{self.base_url}/funds/"
            
            self.logger.info(f"–ü–æ–∏—Å–∫ —Ñ–æ–Ω–¥–∞: {fund_name} -> {clean_name}")
            
            response = self.session.get(funds_url, timeout=15)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # –ò—â–µ–º —Å—Å—ã–ª–∫–∏ –Ω–∞ —Ñ–æ–Ω–¥—ã
            fund_links = soup.find_all('a', href=re.compile(r'/funds/\d+/'))
            
            for link in fund_links:
                try:
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º ID —Ñ–æ–Ω–¥–∞ –∏–∑ URL
                    href = link.get('href', '')
                    fund_id_match = re.search(r'/funds/(\d+)/', href)
                    
                    if not fund_id_match:
                        continue
                    
                    fund_id = int(fund_id_match.group(1))
                    link_text = link.get_text(strip=True)
                    
                    if not link_text:
                        continue
                    
                    # –í—ã—á–∏—Å–ª—è–µ–º —Å—Ö–æ–∂–µ—Å—Ç—å –Ω–∞–∑–≤–∞–Ω–∏–π
                    confidence = self._calculate_name_similarity(clean_name, link_text)
                    
                    if confidence > 0.3:  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —Å—Ö–æ–∂–µ—Å—Ç—å
                        matches.append(FundMatch(
                            fund_id=fund_id,
                            confidence=confidence,
                            name_match=link_text,
                            url=f"{self.base_url}{href}",
                            reason=f"–°—Ö–æ–∂–µ—Å—Ç—å –Ω–∞–∑–≤–∞–Ω–∏—è: {confidence:.2f}"
                        ))
                
                except Exception as e:
                    self.logger.debug(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å—Å—ã–ª–∫–∏ {link}: {e}")
                    continue
            
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
            matches.sort(key=lambda x: x.confidence, reverse=True)
            
            # –ö–µ—à–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            self.search_cache[cache_key] = matches[:5]  # –¢–æ–ø-5 —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π
            
            return matches[:5]
            
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ —Ñ–æ–Ω–¥–∞ {fund_name}: {e}")
            return []
    
    def search_fund_by_ticker_pattern(self, ticker: str, fund_name: str) -> List[FundMatch]:
        """–ò—â–µ—Ç —Ñ–æ–Ω–¥ –ø–æ –ø–∞—Ç—Ç–µ—Ä–Ω–∞–º —Ç–∏–∫–µ—Ä–∞ –≤ –Ω–∞–∑–≤–∞–Ω–∏–∏"""
        
        matches = []
        
        # –°–æ–∑–¥–∞–µ–º –ø–æ–∏—Å–∫–æ–≤—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
        patterns = [
            ticker.upper(),
            ticker.lower(),
            f"({ticker.upper()})",
            f", {ticker.upper()}",
            f" {ticker.upper()} ",
        ]
        
        try:
            # –ü–æ–∏—Å–∫ –≤ –±–∞–∑–µ —Ñ–æ–Ω–¥–æ–≤
            funds_url = f"{self.base_url}/funds/"
            response = self.session.get(funds_url, timeout=15)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            page_text = soup.get_text()
            
            # –ò—â–µ–º —Ç–∏–∫–µ—Ä –≤ —Ç–µ–∫—Å—Ç–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            for pattern in patterns:
                if pattern in page_text:
                    # –ù–∞—Ö–æ–¥–∏–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –≤–æ–∫—Ä—É–≥ —Ç–∏–∫–µ—Ä–∞
                    pattern_pos = page_text.find(pattern)
                    if pattern_pos != -1:
                        context_start = max(0, pattern_pos - 100)
                        context_end = min(len(page_text), pattern_pos + 100)
                        context = page_text[context_start:context_end]
                        
                        # –ò—â–µ–º —Å—Å—ã–ª–∫–∏ –Ω–∞ —Ñ–æ–Ω–¥—ã –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ
                        context_soup = BeautifulSoup(context, 'html.parser')
                        fund_links = context_soup.find_all('a', href=re.compile(r'/funds/\d+/'))
                        
                        for link in fund_links:
                            href = link.get('href', '')
                            fund_id_match = re.search(r'/funds/(\d+)/', href)
                            
                            if fund_id_match:
                                fund_id = int(fund_id_match.group(1))
                                
                                matches.append(FundMatch(
                                    fund_id=fund_id,
                                    confidence=0.8,  # –í—ã—Å–æ–∫–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –ø—Ä–∏ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–∏ —Ç–∏–∫–µ—Ä–∞
                                    name_match=link.get_text(strip=True),
                                    url=f"{self.base_url}{href}",
                                    reason=f"–ù–∞–π–¥–µ–Ω —Ç–∏–∫–µ—Ä {pattern} –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ"
                                ))
                                break
        
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –ø–æ —Ç–∏–∫–µ—Ä—É {ticker}: {e}")
        
        return matches
    
    def _clean_fund_name(self, name: str) -> str:
        """–û—á–∏—â–∞–µ—Ç –Ω–∞–∑–≤–∞–Ω–∏–µ —Ñ–æ–Ω–¥–∞ –¥–ª—è –ø–æ–∏—Å–∫–∞"""
        
        # –£–±–∏—Ä–∞–µ–º –æ–±—â–∏–µ –ø—Ä–µ—Ñ–∏–∫—Å—ã –∏ —Å—É—Ñ—Ñ–∏–∫—Å—ã
        clean = name
        
        # –£–±–∏—Ä–∞–µ–º —É–ø–æ–º–∏–Ω–∞–Ω–∏—è "–ë–ü–ò–§", "ETF", "–§–æ–Ω–¥"
        clean = re.sub(r'\b(–ë–ü–ò–§|ETF|–§–æ–Ω–¥)\b', '', clean, flags=re.IGNORECASE)
        
        # –£–±–∏—Ä–∞–µ–º –∫–∞–≤—ã—á–∫–∏ –∏ —Å–∫–æ–±–∫–∏ —Å —Å–æ–¥–µ—Ä–∂–∏–º—ã–º
        clean = re.sub(r'[¬´¬ª"\'()].*?[¬´¬ª"\'()]', '', clean)
        clean = re.sub(r'\([^)]*\)', '', clean)
        
        # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã
        clean = re.sub(r'\s+', ' ', clean).strip()
        
        return clean
    
    def _calculate_name_similarity(self, name1: str, name2: str) -> float:
        """–í—ã—á–∏—Å–ª—è–µ—Ç —Å—Ö–æ–∂–µ—Å—Ç—å –Ω–∞–∑–≤–∞–Ω–∏–π"""
        
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –Ω–∞–∑–≤–∞–Ω–∏—è
        name1_clean = name1.lower().strip()
        name2_clean = name2.lower().strip()
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º SequenceMatcher –¥–ª—è —Ä–∞—Å—á–µ—Ç–∞ —Å—Ö–æ–∂–µ—Å—Ç–∏
        similarity = difflib.SequenceMatcher(None, name1_clean, name2_clean).ratio()
        
        # –ë–æ–Ω—É—Å –∑–∞ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
        keywords1 = set(re.findall(r'\b\w{3,}\b', name1_clean))
        keywords2 = set(re.findall(r'\b\w{3,}\b', name2_clean))
        
        if keywords1 and keywords2:
            keyword_overlap = len(keywords1 & keywords2) / len(keywords1 | keywords2)
            similarity = max(similarity, keyword_overlap)
        
        return similarity
    
    def verify_fund_match(self, fund_id: int, expected_ticker: str, expected_name: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —á—Ç–æ —Ñ–æ–Ω–¥ –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –æ–∂–∏–¥–∞–Ω–∏—è–º"""
        
        try:
            from investfunds_parser import InvestFundsParser
            parser = InvestFundsParser()
            
            fund_data = parser.get_fund_data(fund_id, use_cache=False)
            
            if not fund_data:
                return False
            
            fund_name = fund_data.get('name', '')
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ —Ç–∏–∫–µ—Ä–∞ –≤ –Ω–∞–∑–≤–∞–Ω–∏–∏
            if expected_ticker.upper() in fund_name.upper():
                return True
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ö–æ–∂–µ—Å—Ç—å –Ω–∞–∑–≤–∞–Ω–∏–π
            similarity = self._calculate_name_similarity(expected_name, fund_name)
            
            return similarity > 0.6
            
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ —Ñ–æ–Ω–¥–∞ {fund_id}: {e}")
            return False
    
    def map_all_funds(self, etf_data: pd.DataFrame, auto_confirm: bool = False) -> Dict[str, int]:
        """–ú–∞–ø–ø–∏—Ç –≤—Å–µ —Ñ–æ–Ω–¥—ã –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏"""
        
        self.logger.info(f"–ù–∞—á–∏–Ω–∞–µ–º –º–∞–ø–ø–∏–Ω–≥ {len(etf_data)} —Ñ–æ–Ω–¥–æ–≤...")
        
        successful_mappings = 0
        
        for idx, row in etf_data.iterrows():
            ticker = row['ticker']
            fund_name = row.get('full_name', row.get('short_name', ticker))
            
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —É–∂–µ –∑–∞–º–∞–ø–ª–µ–Ω–Ω—ã–µ —Ñ–æ–Ω–¥—ã
            if ticker in self.mapping:
                self.logger.info(f"‚úÖ {ticker}: —É–∂–µ –∑–∞–º–∞–ø–ª–µ–Ω (ID: {self.mapping[ticker]})")
                continue
            
            self.logger.info(f"\nüîç –ò—â–µ–º: {ticker} - {fund_name}")
            
            # –ü–æ–∏—Å–∫ –ø–æ –Ω–∞–∑–≤–∞–Ω–∏—é
            name_matches = self.search_fund_by_name(fund_name)
            
            # –ü–æ–∏—Å–∫ –ø–æ —Ç–∏–∫–µ—Ä—É
            ticker_matches = self.search_fund_by_ticker_pattern(ticker, fund_name)
            
            # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            all_matches = name_matches + ticker_matches
            
            # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
            seen_ids = set()
            unique_matches = []
            for match in all_matches:
                if match.fund_id and match.fund_id not in seen_ids:
                    seen_ids.add(match.fund_id)
                    unique_matches.append(match)
            
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
            unique_matches.sort(key=lambda x: x.confidence, reverse=True)
            
            if unique_matches:
                best_match = unique_matches[0]
                
                self.logger.info(f"üìã –ù–∞–π–¥–µ–Ω–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π: {len(unique_matches)}")
                self.logger.info(f"üèÜ –õ—É—á—à–µ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ: ID {best_match.fund_id}, —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å {best_match.confidence:.2f}")
                self.logger.info(f"   –ù–∞–∑–≤–∞–Ω–∏–µ: {best_match.name_match}")
                self.logger.info(f"   –ü—Ä–∏—á–∏–Ω–∞: {best_match.reason}")
                
                # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ –¥–ª—è –≤—ã—Å–æ–∫–æ–π —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
                if auto_confirm and best_match.confidence > 0.8:
                    self.mapping[ticker] = best_match.fund_id
                    successful_mappings += 1
                    self.logger.info(f"‚úÖ –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∑–∞–º–∞–ø–ª–µ–Ω: {ticker} -> {best_match.fund_id}")
                
                elif not auto_confirm:
                    # –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ–µ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ
                    print(f"\nüîç –§–æ–Ω–¥: {ticker} - {fund_name}")
                    print(f"üèÜ –ü—Ä–µ–¥–ª–∞–≥–∞–µ–º—ã–π ID: {best_match.fund_id}")
                    print(f"üìä –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {best_match.confidence:.2f}")
                    print(f"üìù –ù–∞–∑–≤–∞–Ω–∏–µ –Ω–∞ —Å–∞–π—Ç–µ: {best_match.name_match}")
                    print(f"üîó URL: {best_match.url}")
                    
                    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤—ã
                    if len(unique_matches) > 1:
                        print("\nüìã –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤—ã:")
                        for i, match in enumerate(unique_matches[1:4], 2):
                            print(f"   {i}. ID {match.fund_id}: {match.name_match} (—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {match.confidence:.2f})")
                    
                    choice = input("\n–ü–æ–¥—Ç–≤–µ—Ä–∂–¥–∞–µ—Ç–µ –º–∞–ø–ø–∏–Ω–≥? (y/n/id_number): ").strip().lower()
                    
                    if choice == 'y':
                        self.mapping[ticker] = best_match.fund_id
                        successful_mappings += 1
                        self.logger.info(f"‚úÖ –ü–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω –º–∞–ø–ø–∏–Ω–≥: {ticker} -> {best_match.fund_id}")
                    elif choice.isdigit():
                        custom_id = int(choice)
                        self.mapping[ticker] = custom_id
                        successful_mappings += 1
                        self.logger.info(f"‚úÖ –†—É—á–Ω–æ–π –º–∞–ø–ø–∏–Ω–≥: {ticker} -> {custom_id}")
                    else:
                        self.logger.info(f"‚ùå –ü—Ä–æ–ø—É—â–µ–Ω: {ticker}")
            else:
                self.logger.warning(f"‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π –¥–ª—è: {ticker}")
            
            # –ü–∞—É–∑–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
            time.sleep(1)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å –∫–∞–∂–¥—ã–µ 10 —Ñ–æ–Ω–¥–æ–≤
            if (idx + 1) % 10 == 0:
                self.save_mapping()
                self.logger.info(f"üíæ –ü—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ: {successful_mappings}/{idx+1}")
        
        # –§–∏–Ω–∞–ª—å–Ω–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
        self.save_mapping()
        
        self.logger.info(f"\nüéâ –ú–∞–ø–ø–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω!")
        self.logger.info(f"‚úÖ –£—Å–ø–µ—à–Ω–æ –∑–∞–º–∞–ø–ª–µ–Ω–æ: {successful_mappings}")
        self.logger.info(f"‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–æ: {len(etf_data) - successful_mappings}")
        self.logger.info(f"üìä –ü—Ä–æ—Ü–µ–Ω—Ç —É—Å–ø–µ—Ö–∞: {successful_mappings/len(etf_data)*100:.1f}%")
        
        return self.mapping

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –º–∞–ø–ø–∏–Ω–≥–∞"""
    
    import argparse
    
    parser = argparse.ArgumentParser(description='–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –º–∞–ø–ø–∏–Ω–≥ –ë–ü–ò–§ –Ω–∞ investfunds.ru')
    parser.add_argument('--auto', action='store_true', help='–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ –¥–ª—è –≤—ã—Å–æ–∫–æ–π —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏')
    parser.add_argument('--data-file', help='–ü—É—Ç—å –∫ —Ñ–∞–π–ª—É —Å –¥–∞–Ω–Ω—ã–º–∏ ETF')
    
    args = parser.parse_args()
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ ETF
    if args.data_file:
        data_file = args.data_file
    else:
        # –ò—â–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π —Ñ–∞–π–ª —Å –¥–∞–Ω–Ω—ã–º–∏
        data_files = list(Path('.').glob('enhanced_etf_data_*.csv'))
        if not data_files:
            data_files = list(Path('.').glob('full_moex_etf_data_*.csv'))
        
        if not data_files:
            print("‚ùå –§–∞–π–ª—ã —Å –¥–∞–Ω–Ω—ã–º–∏ ETF –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
            return
        
        data_file = max(data_files, key=lambda x: x.stat().st_mtime)
    
    print(f"üìä –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ {data_file}")
    etf_data = pd.read_csv(data_file)
    print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(etf_data)} ETF")
    
    # –°–æ–∑–¥–∞–µ–º –º–∞–ø–ø–µ—Ä
    mapper = FundMapper()
    
    print(f"\nüöÄ –ù–∞—á–∏–Ω–∞–µ–º –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –º–∞–ø–ø–∏–Ω–≥ –≤—Å–µ—Ö {len(etf_data)} —Ñ–æ–Ω–¥–æ–≤")
    print(f"‚öôÔ∏è  –†–µ–∂–∏–º: {'–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π' if args.auto else '–ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π'}")
    
    # –ó–∞–ø—É—Å–∫–∞–µ–º –º–∞–ø–ø–∏–Ω–≥
    final_mapping = mapper.map_all_funds(etf_data, auto_confirm=args.auto)
    
    print(f"\nüìã –§–∏–Ω–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
    print(f"   –í—Å–µ–≥–æ —Ñ–æ–Ω–¥–æ–≤: {len(etf_data)}")
    print(f"   –ó–∞–º–∞–ø–ª–µ–Ω–æ: {len(final_mapping)}")
    print(f"   –ù–µ –Ω–∞–π–¥–µ–Ω–æ: {len(etf_data) - len(final_mapping)}")
    print(f"   –§–∞–π–ª –º–∞–ø–ø–∏–Ω–≥–∞: fund_mapping.json")

if __name__ == "__main__":
    main()