#!/usr/bin/env python3
"""
–ü—Ä—è–º–æ–π –ø–æ–∏—Å–∫ —Ñ–æ–Ω–¥–æ–≤ –Ω–∞ investfunds.ru –ø–æ —Ç–∏–∫–µ—Ä—É –∏ –ø—Ä–æ–≤–µ—Ä–∫–∞ –≤—Å–µ—Ö –≤–æ–∑–º–æ–∂–Ω—ã—Ö ID
"""

import requests
from bs4 import BeautifulSoup
import re
import pandas as pd
from typing import Dict, List, Optional
import time
from pathlib import Path
import json

class DirectInvestFundsSearch:
    """–ü—Ä—è–º–æ–π –ø–æ–∏—Å–∫ –Ω–∞ investfunds.ru"""
    
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
        })
        self.base_url = "https://investfunds.ru"
        
    def search_by_ticker(self, ticker: str) -> List[Dict]:
        """–ò—â–µ—Ç —Ñ–æ–Ω–¥ –ø–æ —Ç–∏–∫–µ—Ä—É —á–µ—Ä–µ–∑ –ø–æ–∏—Å–∫–æ–≤—É—é —Å—Ç—Ä–æ–∫—É"""
        
        results = []
        
        try:
            # –ü–æ–∏—Å–∫ —á–µ—Ä–µ–∑ –æ—Å–Ω–æ–≤–Ω—É—é –ø–æ–∏—Å–∫–æ–≤—É—é —Å—Ç—Ä–æ–∫—É
            search_url = f"{self.base_url}/search/?q={ticker}"
            
            response = self.session.get(search_url, timeout=15)
            if response.status_code != 200:
                return results
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # –ò—â–µ–º —Å—Å—ã–ª–∫–∏ –Ω–∞ —Ñ–æ–Ω–¥—ã –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞—Ö –ø–æ–∏—Å–∫–∞
            fund_links = soup.find_all('a', href=re.compile(r'/funds/\d+/'))
            
            for link in fund_links:
                href = link.get('href', '')
                fund_id_match = re.search(r'/funds/(\d+)/', href)
                
                if fund_id_match:
                    fund_id = int(fund_id_match.group(1))
                    link_text = link.get_text(strip=True)
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Å—Å—ã–ª–∫–∞ —Å–æ–¥–µ—Ä–∂–∏—Ç —Ç–∏–∫–µ—Ä
                    if ticker.upper() in link_text.upper():
                        results.append({
                            'fund_id': fund_id,
                            'name': link_text,
                            'url': f"{self.base_url}{href}",
                            'confidence': 0.9,
                            'source': 'search'
                        })
            
            # –ü–æ–∏—Å–∫ –≤ —Ç–µ–∫—Å—Ç–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            page_text = soup.get_text()
            ticker_mentions = page_text.upper().count(ticker.upper())
            
            if ticker_mentions > 0:
                print(f"üîç –¢–∏–∫–µ—Ä {ticker} —É–ø–æ–º–∏–Ω–∞–µ—Ç—Å—è {ticker_mentions} —Ä–∞–∑ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ –ø–æ–∏—Å–∫–∞")
                
                # –ò—â–µ–º ID —Ä—è–¥–æ–º —Å —É–ø–æ–º–∏–Ω–∞–Ω–∏—è–º–∏ —Ç–∏–∫–µ—Ä–∞
                pattern = rf'{ticker.upper()}.*?/funds/(\d+)/'
                id_matches = re.findall(pattern, page_text.upper())
                
                for found_id in id_matches:
                    if not any(r['fund_id'] == int(found_id) for r in results):
                        results.append({
                            'fund_id': int(found_id),
                            'name': f'Found via pattern {ticker}',
                            'url': f"{self.base_url}/funds/{found_id}/",
                            'confidence': 0.7,
                            'source': 'pattern'
                        })
            
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ {ticker}: {e}")
        
        return results
    
    def scan_fund_id_range(self, start_id: int = 1, end_id: int = 15000, ticker_filter: str = None) -> List[Dict]:
        """–°–∫–∞–Ω–∏—Ä—É–µ—Ç –¥–∏–∞–ø–∞–∑–æ–Ω ID —Ñ–æ–Ω–¥–æ–≤ –¥–ª—è –ø–æ–∏—Å–∫–∞ –Ω—É–∂–Ω–æ–≥–æ —Ç–∏–∫–µ—Ä–∞"""
        
        found_funds = []
        
        print(f"üîç –°–∫–∞–Ω–∏—Ä—É–µ–º ID —Ñ–æ–Ω–¥–æ–≤ –æ—Ç {start_id} –¥–æ {end_id}")
        
        for fund_id in range(start_id, end_id + 1):
            try:
                url = f"{self.base_url}/funds/{fund_id}/"
                response = self.session.get(url, timeout=10)
                
                if response.status_code == 200:
                    soup = BeautifulSoup(response.content, 'html.parser')
                    page_text = soup.get_text().upper()
                    
                    # –ï—Å–ª–∏ —É–∫–∞–∑–∞–Ω —Ñ–∏–ª—å—Ç—Ä –ø–æ —Ç–∏–∫–µ—Ä—É
                    if ticker_filter and ticker_filter.upper() in page_text:
                        
                        # –ò–∑–≤–ª–µ–∫–∞–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ —Ñ–æ–Ω–¥–∞
                        title = soup.find('h1')
                        fund_name = title.get_text(strip=True) if title else 'Unknown'
                        
                        # –ò—â–µ–º –°–ß–ê
                        nav_pattern = r'(\d{1,3}(?:,\d{3})*(?:\.\d+)?)'
                        nav_matches = re.findall(nav_pattern, page_text)
                        
                        nav_value = 0
                        for match in nav_matches:
                            try:
                                value = float(match.replace(',', ''))
                                if value > 1000000:  # –ë–æ–ª—å—à–µ –º–∏–ª–ª–∏–æ–Ω–∞ - –ø–æ—Ö–æ–∂–µ –Ω–∞ –°–ß–ê
                                    nav_value = value
                                    break
                            except:
                                continue
                        
                        found_funds.append({
                            'fund_id': fund_id,
                            'ticker': ticker_filter,
                            'name': fund_name,
                            'nav': nav_value,
                            'url': url,
                            'confidence': 1.0
                        })
                        
                        print(f"‚úÖ –ù–∞–π–¥–µ–Ω {ticker_filter}: ID {fund_id}, –°–ß–ê {nav_value:,.0f}")
                    
                    # –ï—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω —Ñ–∏–ª—å—Ç—Ä, —Å–æ–±–∏—Ä–∞–µ–º –≤—Å–µ —Ñ–æ–Ω–¥—ã
                    elif not ticker_filter:
                        title = soup.find('h1')
                        if title:
                            fund_name = title.get_text(strip=True)
                            found_funds.append({
                                'fund_id': fund_id,
                                'name': fund_name,
                                'url': url
                            })
                
                # –ü–∞—É–∑–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
                if fund_id % 100 == 0:
                    print(f"üìä –ü—Ä–æ–≤–µ—Ä–µ–Ω–æ {fund_id} ID...")
                    time.sleep(1)
                
                time.sleep(0.1)
                
            except Exception as e:
                if fund_id % 1000 == 0:
                    print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –Ω–∞ ID {fund_id}: {e}")
                continue
        
        return found_funds
    
    def test_known_funds(self) -> Dict[str, Dict]:
        """–¢–µ—Å—Ç–∏—Ä—É–µ—Ç –ø–æ–∏—Å–∫ –¥–ª—è –∏–∑–≤–µ—Å—Ç–Ω—ã—Ö —Ñ–æ–Ω–¥–æ–≤"""
        
        # –ò–∑–≤–µ—Å—Ç–Ω—ã–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
        known_funds = {
            'AKMB': 6225,  # –ò–∑ –≤–∞—à–µ–≥–æ –ø—Ä–∏–º–µ—Ä–∞
            'LQDT': 5973,  # –£–∂–µ –∑–Ω–∞–µ–º
        }
        
        results = {}
        
        for ticker, expected_id in known_funds.items():
            print(f"\nüß™ –¢–µ—Å—Ç–∏—Ä—É–µ–º {ticker} (–æ–∂–∏–¥–∞–µ–º—ã–π ID: {expected_id})")
            
            # –ü—Ä—è–º–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –∏–∑–≤–µ—Å—Ç–Ω–æ–≥–æ ID
            url = f"{self.base_url}/funds/{expected_id}/"
            
            try:
                response = self.session.get(url, timeout=15)
                if response.status_code == 200:
                    soup = BeautifulSoup(response.content, 'html.parser')
                    page_text = soup.get_text()
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ —Ç–∏–∫–µ—Ä –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ
                    if ticker.upper() in page_text.upper():
                        
                        # –ò–∑–≤–ª–µ–∫–∞–µ–º –°–ß–ê
                        nav_patterns = [
                            r'(\d{1,3}(?:,\d{3})*(?:\.\d+)?)',
                            r'–°–ß–ê[:\s]*([0-9,.\s]+)',
                        ]
                        
                        nav_value = 0
                        for pattern in nav_patterns:
                            matches = re.findall(pattern, page_text)
                            for match in matches:
                                try:
                                    cleaned = re.sub(r'[^\d.]', '', match)
                                    value = float(cleaned)
                                    if value > 1000000:
                                        nav_value = value
                                        break
                                except:
                                    continue
                            if nav_value > 0:
                                break
                        
                        results[ticker] = {
                            'fund_id': expected_id,
                            'nav': nav_value,
                            'url': url,
                            'status': 'confirmed'
                        }
                        
                        print(f"  ‚úÖ –ü–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω: –°–ß–ê {nav_value:,.0f}")
                    else:
                        print(f"  ‚ùå –¢–∏–∫–µ—Ä {ticker} –Ω–µ –Ω–∞–π–¥–µ–Ω –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ ID {expected_id}")
                else:
                    print(f"  ‚ùå –°—Ç—Ä–∞–Ω–∏—Ü–∞ ID {expected_id} –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞")
                    
            except Exception as e:
                print(f"  ‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ {ticker}: {e}")
        
        return results

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è"""
    
    searcher = DirectInvestFundsSearch()
    
    print("üéØ –ü–†–Ø–ú–û–ô –ü–û–ò–°–ö –ù–ê INVESTFUNDS.RU")
    print("=" * 50)
    
    # 1. –¢–µ—Å—Ç–∏—Ä—É–µ–º –∏–∑–≤–µ—Å—Ç–Ω—ã–µ —Ñ–æ–Ω–¥—ã
    print("\n1Ô∏è‚É£ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∏–∑–≤–µ—Å—Ç–Ω—ã—Ö —Ñ–æ–Ω–¥–æ–≤:")
    known_results = searcher.test_known_funds()
    
    # 2. –ü–æ–∏—Å–∫ –ø–æ —Ç–∏–∫–µ—Ä—É AKMB
    print("\n2Ô∏è‚É£ –ü–æ–∏—Å–∫ AKMB —á–µ—Ä–µ–∑ –ø–æ–∏—Å–∫–æ–≤—É—é —Å—Ç—Ä–æ–∫—É:")
    akmb_search = searcher.search_by_ticker('AKMB')
    
    for result in akmb_search:
        print(f"  –ù–∞–π–¥–µ–Ω: ID {result['fund_id']}, {result['name']}")
    
    # 3. –¶–µ–ª–µ–≤–æ–π –ø–æ–∏—Å–∫ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ —Ñ–æ–Ω–¥–∞
    print("\n3Ô∏è‚É£ –¶–µ–ª–µ–≤–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ AKMB –Ω–∞ ID 6225:")
    
    try:
        from investfunds_parser import InvestFundsParser
        parser = InvestFundsParser()
        
        # –î–æ–±–∞–≤–ª—è–µ–º AKMB –≤ –º–∞–ø–ø–∏–Ω–≥ –∏ —Ç–µ—Å—Ç–∏—Ä—É–µ–º
        akmb_data = parser.get_fund_data(6225, use_cache=False)
        
        if akmb_data:
            print(f"  ‚úÖ –£—Å–ø–µ—à–Ω–æ –ø–æ–ª—É—á–µ–Ω—ã –¥–∞–Ω–Ω—ã–µ:")
            print(f"     –ù–∞–∑–≤–∞–Ω–∏–µ: {akmb_data['name']}")
            print(f"     –°–ß–ê: {akmb_data['nav']:,.2f} —Ä—É–±.")
            print(f"     –¶–µ–Ω–∞ –ø–∞—è: {akmb_data['unit_price']:.4f} —Ä—É–±.")
        else:
            print(f"  ‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –¥–∞–Ω–Ω—ã–µ")
            
    except Exception as e:
        print(f"  ‚ö†Ô∏è –û—à–∏–±–∫–∞ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –ø–∞—Ä—Å–µ—Ä–∞: {e}")
    
    print(f"\nüìä –ò—Ç–æ–≥–∏:")
    print(f"  –ò–∑–≤–µ—Å—Ç–Ω—ã—Ö —Ñ–æ–Ω–¥–æ–≤ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–æ: {len(known_results)}")
    print(f"  –†–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞ AKMB: {len(akmb_search)}")

if __name__ == "__main__":
    main()