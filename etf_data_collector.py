"""
ETF Data Collector with Fallback - ĞºĞ¾Ğ»Ğ»ĞµĞºÑ‚Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾ Ñ€Ğ¾ÑÑĞ¸Ğ¹ÑĞºĞ¸Ñ… Ğ‘ĞŸĞ˜Ğ¤Ğ°Ñ… Ñ fallback ÑĞ¸ÑÑ‚ĞµĞ¼Ğ¾Ğ¹
"""

import pandas as pd
from datetime import datetime
from typing import Dict, List, Optional
import time

from fallback_manager import DataProviderManager
from fallback_system import AllProvidersUnavailableError
from config import KNOWN_ETFS
from logger_config import logger, log_performance


class ETFDataCollectorWithFallback:
    """
    ĞšĞ¾Ğ»Ğ»ĞµĞºÑ‚Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾ Ñ€Ğ¾ÑÑĞ¸Ğ¹ÑĞºĞ¸Ñ… Ğ‘ĞŸĞ˜Ğ¤Ğ°Ñ… Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ fallback ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹
    Ğ Ğ°ÑÑˆĞ¸Ñ€ÑĞµÑ‚ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰ĞµĞ³Ğ¾ BaseETFCollector
    """
    
    def __init__(self):
        self.fallback_manager = DataProviderManager()
        self.known_etfs = KNOWN_ETFS
        
        logger.info("ETF Data Collector Ñ fallback ÑĞ¸ÑÑ‚ĞµĞ¼Ğ¾Ğ¹ Ğ¸Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½")
        logger.info(f"Ğ˜Ğ·Ğ²ĞµÑÑ‚Ğ½Ñ‹Ñ… Ğ‘ĞŸĞ˜Ğ¤Ğ¾Ğ² Ğ² Ğ±Ğ°Ğ·Ğµ: {len(self.known_etfs)}")
    
    @log_performance
    def collect_all_etf_data(self) -> pd.DataFrame:
        """
        Ğ¡Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ Ğ²ÑĞµĞ¼ Ğ¸Ğ·Ğ²ĞµÑÑ‚Ğ½Ñ‹Ğ¼ Ñ€Ğ¾ÑÑĞ¸Ğ¹ÑĞºĞ¸Ğ¼ Ğ‘ĞŸĞ˜Ğ¤Ğ°Ğ¼
        
        Returns:
            pd.DataFrame: Ğ”Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾ Ğ²ÑĞµĞ¼ Ğ‘ĞŸĞ˜Ğ¤Ğ°Ğ¼ Ñ Ğ¼ĞµÑ‚Ğ°Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸ĞµĞ¹
        """
        logger.info("ĞĞ°Ñ‡Ğ¸Ğ½Ğ°ĞµĞ¼ ÑĞ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ Ğ²ÑĞµĞ¼ Ñ€Ğ¾ÑÑĞ¸Ğ¹ÑĞºĞ¸Ğ¼ Ğ‘ĞŸĞ˜Ğ¤Ğ°Ğ¼")
        
        etf_data_list = []
        successful_collections = 0
        failed_collections = 0
        
        for ticker, metadata in self.known_etfs.items():
            logger.info(f"ĞĞ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° Ğ‘ĞŸĞ˜Ğ¤Ğ°: {ticker} ({metadata.get('name', 'ĞĞµĞ¸Ğ·Ğ²ĞµÑÑ‚Ğ½Ğ¾')})")
            
            try:
                # ĞŸĞ¾Ğ»ÑƒÑ‡Ğ°ĞµĞ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ fallback
                result = self.fallback_manager.get_etf_data_with_fallback(ticker)
                
                # ĞĞ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµĞ¼ Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ Ğ¼ĞµÑ‚Ğ°Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸
                etf_record = self._merge_data_with_metadata(ticker, result.data, metadata)
                
                # Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµĞ¼ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾ fallback
                etf_record.update({
                    'data_source': result.source,
                    'fallback_level': result.fallback_level,
                    'data_quality_score': result.quality_score,
                    'collection_timestamp': datetime.now().isoformat(),
                    'warnings': result.warnings
                })
                
                etf_data_list.append(etf_record)
                successful_collections += 1
                
                logger.info(f"âœ“ {ticker}: Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ñ‹ Ğ¸Ğ· {result.source} (ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾: {result.quality_score:.2f})")
                
            except AllProvidersUnavailableError as e:
                logger.error(f"âœ— {ticker}: Ğ²ÑĞµ Ğ¿Ñ€Ğ¾Ğ²Ğ°Ğ¹Ğ´ĞµÑ€Ñ‹ Ğ½ĞµĞ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹ - {e}")
                
                # Ğ¡Ğ¾Ğ·Ğ´Ğ°ĞµĞ¼ Ğ·Ğ°Ğ¿Ğ¸ÑÑŒ Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸
                etf_record = self._create_fallback_record(ticker, metadata)
                etf_data_list.append(etf_record)
                failed_collections += 1
                
            except Exception as e:
                logger.error(f"âœ— {ticker}: Ğ½ĞµĞ¾Ğ¶Ğ¸Ğ´Ğ°Ğ½Ğ½Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° - {e}")
                
                etf_record = self._create_fallback_record(ticker, metadata)
                etf_data_list.append(etf_record)
                failed_collections += 1
            
            # ĞĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ°Ñ Ğ¿Ğ°ÑƒĞ·Ğ° Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°Ğ¼Ğ¸
            time.sleep(0.5)
        
        # Ğ¡Ğ¾Ğ·Ğ´Ğ°ĞµĞ¼ DataFrame
        df = pd.DataFrame(etf_data_list)
        
        # Ğ Ğ°ÑÑÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ĞµĞ¼ Ñ€Ñ‹Ğ½Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ğ´Ğ¾Ğ»Ğ¸
        df = self._calculate_market_shares(df)
        
        # Ğ›Ğ¾Ğ³Ğ¸Ñ€ÑƒĞµĞ¼ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹
        logger.info(f"Ğ¡Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½:")
        logger.info(f"  âœ“ Ğ£ÑĞ¿ĞµÑˆĞ½Ğ¾: {successful_collections}")
        logger.info(f"  âœ— ĞĞµÑƒĞ´Ğ°Ñ‡Ğ½Ğ¾: {failed_collections}")
        logger.info(f"  ğŸ“Š Ğ’ÑĞµĞ³Ğ¾ Ğ‘ĞŸĞ˜Ğ¤Ğ¾Ğ²: {len(df)}")
        
        return df
    
    def collect_etf_data(self, ticker: str) -> Dict:
        """
        Ğ¡Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ğ¾Ğ¼Ñƒ Ğ‘ĞŸĞ˜Ğ¤Ñƒ
        
        Args:
            ticker: Ğ¢Ğ¸ĞºĞµÑ€ Ğ‘ĞŸĞ˜Ğ¤Ğ°
            
        Returns:
            Dict: Ğ”Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¾ Ğ‘ĞŸĞ˜Ğ¤Ğµ
        """
        logger.info(f"Ğ¡Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ Ğ‘ĞŸĞ˜Ğ¤Ñƒ: {ticker}")
        
        try:
            result = self.fallback_manager.get_etf_data_with_fallback(ticker)
            
            metadata = self.known_etfs.get(ticker, {})
            etf_data = self._merge_data_with_metadata(ticker, result.data, metadata)
            
            etf_data.update({
                'data_source': result.source,
                'fallback_level': result.fallback_level,
                'data_quality_score': result.quality_score,
                'collection_timestamp': datetime.now().isoformat()
            })
            
            logger.info(f"Ğ”Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾ {ticker} Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ñ‹ Ğ¸Ğ· {result.source}")
            return etf_data
            
        except Exception as e:
            logger.error(f"ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ {ticker}: {e}")
            return self._create_fallback_record(ticker, self.known_etfs.get(ticker, {}))
    
    def get_etf_list(self) -> List[Dict]:
        """
        ĞŸĞ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ğµ ÑĞ¿Ğ¸ÑĞºĞ° Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹Ñ… Ğ‘ĞŸĞ˜Ğ¤Ğ¾Ğ²
        
        Returns:
            List[Dict]: Ğ¡Ğ¿Ğ¸ÑĞ¾Ğº Ğ‘ĞŸĞ˜Ğ¤Ğ¾Ğ² Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸ĞµĞ¹
        """
        logger.info("ĞŸĞ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ğµ ÑĞ¿Ğ¸ÑĞºĞ° Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹Ñ… Ğ‘ĞŸĞ˜Ğ¤Ğ¾Ğ²")
        
        try:
            result = self.fallback_manager.get_etf_list_with_fallback()
            
            # ĞĞ±Ğ¾Ğ³Ğ°Ñ‰Ğ°ĞµĞ¼ ÑĞ¿Ğ¸ÑĞ¾Ğº Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸Ğ· Ğ½Ğ°ÑˆĞµĞ¹ Ğ±Ğ°Ğ·Ñ‹ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹
            enriched_list = []
            for security in result.data['securities']:
                ticker = security.get('ticker')
                if ticker in self.known_etfs:
                    security.update(self.known_etfs[ticker])
                enriched_list.append(security)
            
            logger.info(f"ĞŸĞ¾Ğ»ÑƒÑ‡ĞµĞ½ ÑĞ¿Ğ¸ÑĞ¾Ğº Ğ¸Ğ· {len(enriched_list)} Ğ‘ĞŸĞ˜Ğ¤Ğ¾Ğ² Ğ¸Ğ· {result.source}")
            return enriched_list
            
        except Exception as e:
            logger.error(f"ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¸ÑĞºĞ° Ğ‘ĞŸĞ˜Ğ¤Ğ¾Ğ²: {e}")
            
            # Ğ’Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµĞ¼ ÑĞ¿Ğ¸ÑĞ¾Ğº Ğ¸Ğ· Ğ½Ğ°ÑˆĞµĞ¹ Ğ±Ğ°Ğ·Ñ‹ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹
            fallback_list = []
            for ticker, metadata in self.known_etfs.items():
                fallback_list.append({
                    'ticker': ticker,
                    'name': metadata.get('name', ticker),
                    'management_company': metadata.get('management_company', 'ĞĞµĞ¸Ğ·Ğ²ĞµÑÑ‚Ğ½Ğ¾'),
                    'category': metadata.get('category', 'ĞĞµĞ¸Ğ·Ğ²ĞµÑÑ‚Ğ½Ğ¾'),
                    'source': 'fallback_knowledge_base'
                })
            
            logger.info(f"Ğ’Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰ĞµĞ½ fallback ÑĞ¿Ğ¸ÑĞ¾Ğº Ğ¸Ğ· {len(fallback_list)} Ğ‘ĞŸĞ˜Ğ¤Ğ¾Ğ²")
            return fallback_list
    
    def get_macro_data(self) -> Dict:
        """
        ĞŸĞ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¼Ğ°ĞºÑ€Ğ¾ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…
        
        Returns:
            Dict: ĞœĞ°ĞºÑ€Ğ¾ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»Ğ¸
        """
        logger.info("ĞŸĞ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¼Ğ°ĞºÑ€Ğ¾ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…")
        
        try:
            result = self.fallback_manager.get_macro_data_with_fallback()
            
            macro_data = result.data.copy()
            macro_data.update({
                'data_source': result.source,
                'quality_score': result.quality_score,
                'collection_timestamp': datetime.now().isoformat()
            })
            
            logger.info(f"ĞœĞ°ĞºÑ€Ğ¾ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ñ‹ Ğ¸Ğ· {result.source}")
            return macro_data
            
        except Exception as e:
            logger.error(f"ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ°ĞºÑ€Ğ¾ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…: {e}")
            return {
                'currency_rates': {},
                'key_rate': None,
                'error': str(e),
                'data_source': 'error',
                'collection_timestamp': datetime.now().isoformat()
            }
    
    def get_provider_status(self) -> Dict:
        """
        ĞŸĞ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ğµ ÑÑ‚Ğ°Ñ‚ÑƒÑĞ° Ğ²ÑĞµÑ… Ğ¿Ñ€Ğ¾Ğ²Ğ°Ğ¹Ğ´ĞµÑ€Ğ¾Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…
        
        Returns:
            Dict: Ğ¡Ñ‚Ğ°Ñ‚ÑƒÑ Ğ¿Ñ€Ğ¾Ğ²Ğ°Ğ¹Ğ´ĞµÑ€Ğ¾Ğ²
        """
        return self.fallback_manager.get_provider_status()
    
    def _merge_data_with_metadata(self, ticker: str, api_data: Dict, metadata: Dict) -> Dict:
        """
        ĞĞ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· API Ñ Ğ¼ĞµÑ‚Ğ°Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸
        """
        merged_data = {
            'ticker': ticker,
            'name': metadata.get('name', ticker),
            'management_company': metadata.get('management_company', 'ĞĞµĞ¸Ğ·Ğ²ĞµÑÑ‚Ğ½Ğ¾'),
            'category': metadata.get('category', 'ĞĞµĞ¸Ğ·Ğ²ĞµÑÑ‚Ğ½Ğ¾'),
            'tracking_index': metadata.get('tracking_index', 'ĞĞµĞ¸Ğ·Ğ²ĞµÑÑ‚Ğ½Ğ¾'),
            'inception_year': metadata.get('inception_year'),
            'expense_ratio': metadata.get('expense_ratio'),
            'benchmark': metadata.get('benchmark')
        }
        
        # Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµĞ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸Ğ· API
        merged_data.update(api_data)
        
        # ĞĞ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·ÑƒĞµĞ¼ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ¿Ğ¾Ğ»Ñ
        merged_data['current_price'] = (
            api_data.get('current_price') or 
            api_data.get('last_price') or 
            api_data.get('regularMarketPrice')
        )
        
        merged_data['annual_return'] = (
            api_data.get('return_annualized') or 
            api_data.get('return_1y') or 
            api_data.get('return_period')
        )
        
        merged_data['volatility'] = api_data.get('volatility')
        merged_data['daily_volume'] = api_data.get('avg_daily_volume')
        merged_data['daily_value_rub'] = api_data.get('avg_daily_value_rub')
        
        return merged_data
    
    def _create_fallback_record(self, ticker: str, metadata: Dict) -> Dict:
        """
        Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ·Ğ°Ğ¿Ğ¸ÑĞ¸ Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€Ğ¸ Ğ½ĞµĞ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ¾ÑÑ‚Ğ¸ API
        """
        return {
            'ticker': ticker,
            'name': metadata.get('name', ticker),
            'management_company': metadata.get('management_company', 'ĞĞµĞ¸Ğ·Ğ²ĞµÑÑ‚Ğ½Ğ¾'),
            'category': metadata.get('category', 'ĞĞµĞ¸Ğ·Ğ²ĞµÑÑ‚Ğ½Ğ¾'),
            'tracking_index': metadata.get('tracking_index', 'ĞĞµĞ¸Ğ·Ğ²ĞµÑÑ‚Ğ½Ğ¾'),
            'inception_year': metadata.get('inception_year'),
            'expense_ratio': metadata.get('expense_ratio'),
            'benchmark': metadata.get('benchmark'),
            'current_price': None,
            'annual_return': None,
            'volatility': None,
            'daily_volume': None,
            'daily_value_rub': None,
            'data_source': 'fallback_metadata',
            'fallback_level': 99,
            'data_quality_score': 0.1,
            'collection_timestamp': datetime.now().isoformat(),
            'warnings': ['Ğ”Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ½ĞµĞ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹ Ğ¸Ğ· Ğ²ÑĞµÑ… Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¾Ğ²'],
            'data_availability': 'metadata_only'
        }
    
    def _calculate_market_shares(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        Ğ Ğ°ÑÑ‡ĞµÑ‚ Ñ€Ñ‹Ğ½Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ´Ğ¾Ğ»ĞµĞ¹ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‰Ğ¸Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ°Ğ½Ğ¸Ğ¹
        """
        logger.info("Ğ Ğ°ÑÑ‡ĞµÑ‚ Ñ€Ñ‹Ğ½Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ´Ğ¾Ğ»ĞµĞ¹ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‰Ğ¸Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ°Ğ½Ğ¸Ğ¹")
        
        # Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€ÑƒĞµĞ¼ Ğ·Ğ°Ğ¿Ğ¸ÑĞ¸ Ñ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¾Ğ± Ğ¾Ğ±ÑŠĞµĞ¼Ğ°Ñ…
        valid_data = df[
            (df['daily_value_rub'].notna()) & 
            (df['daily_value_rub'] > 0)
        ].copy()
        
        if len(valid_data) == 0:
            logger.warning("ĞĞµÑ‚ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾Ğ± Ğ¾Ğ±ÑŠĞµĞ¼Ğ°Ñ… Ğ´Ğ»Ñ Ñ€Ğ°ÑÑ‡ĞµÑ‚Ğ° Ñ€Ñ‹Ğ½Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ´Ğ¾Ğ»ĞµĞ¹")
            df['market_share_percent'] = None
            return df
        
        # Ğ Ğ°ÑÑÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ĞµĞ¼ Ğ¿Ñ€Ğ¸Ğ±Ğ»Ğ¸Ğ·Ğ¸Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ Ñ€Ñ‹Ğ½Ğ¾Ñ‡Ğ½ÑƒÑ ĞºĞ°Ğ¿Ğ¸Ñ‚Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ
        # Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ Ğ´Ğ½ĞµĞ²Ğ½Ñ‹Ğµ Ğ¾Ğ±ÑŠĞµĞ¼Ñ‹ Ñ‚Ğ¾Ñ€Ğ³Ğ¾Ğ² ĞºĞ°Ğº Ğ¿Ñ€Ğ¾ĞºÑĞ¸
        valid_data['estimated_market_cap'] = valid_data['daily_value_rub'] * 30  # ĞŸÑ€Ğ¸Ğ¼ĞµÑ€Ğ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ°
        
        # Ğ“Ñ€ÑƒĞ¿Ğ¿Ğ¸Ñ€ÑƒĞµĞ¼ Ğ¿Ğ¾ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‰Ğ¸Ğ¼ ĞºĞ¾Ğ¼Ğ¿Ğ°Ğ½Ğ¸ÑĞ¼
        uk_totals = valid_data.groupby('management_company')['estimated_market_cap'].sum()
        total_market = uk_totals.sum()
        
        if total_market > 0:
            uk_shares = (uk_totals / total_market * 100).round(1)
            
            # Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµĞ¼ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾ Ğ´Ğ¾Ğ»ÑÑ… Ñ€Ñ‹Ğ½ĞºĞ°
            df['market_share_percent'] = df['management_company'].map(uk_shares)
            
            logger.info("Ğ Ñ‹Ğ½Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ğ´Ğ¾Ğ»Ğ¸ Ñ€Ğ°ÑÑÑ‡Ğ¸Ñ‚Ğ°Ğ½Ñ‹:")
            for uk, share in uk_shares.items():
                logger.info(f"  {uk}: {share}%")
        else:
            df['market_share_percent'] = None
        
        return df
    
    @log_performance
    def create_comprehensive_report(self) -> Dict:
        """
        Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ñ‚Ñ‡ĞµÑ‚Ğ° Ğ¿Ğ¾ Ñ€Ğ¾ÑÑĞ¸Ğ¹ÑĞºĞ¸Ğ¼ Ğ‘ĞŸĞ˜Ğ¤Ğ°Ğ¼
        
        Returns:
            Dict: ĞšĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ¾Ñ‚Ñ‡ĞµÑ‚
        """
        logger.info("Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ñ‚Ñ‡ĞµÑ‚Ğ° Ğ¿Ğ¾ Ñ€Ğ¾ÑÑĞ¸Ğ¹ÑĞºĞ¸Ğ¼ Ğ‘ĞŸĞ˜Ğ¤Ğ°Ğ¼")
        
        # Ğ¡Ğ¾Ğ±Ğ¸Ñ€Ğ°ĞµĞ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ
        etf_df = self.collect_all_etf_data()
        macro_data = self.get_macro_data()
        provider_status = self.get_provider_status()
        
        # ĞĞ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµĞ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ
        report = {
            'report_metadata': {
                'creation_date': datetime.now().isoformat(),
                'total_etfs_analyzed': len(etf_df),
                'data_sources_used': etf_df['data_source'].value_counts().to_dict(),
                'average_data_quality': etf_df['data_quality_score'].mean()
            },
            
            'market_overview': {
                'total_etfs': len(etf_df),
                'management_companies': etf_df['management_company'].nunique(),
                'categories': etf_df['category'].value_counts().to_dict(),
                'market_shares': etf_df.groupby('management_company')['market_share_percent'].first().dropna().to_dict()
            },
            
            'performance_analysis': self._analyze_performance(etf_df),
            'risk_analysis': self._analyze_risk(etf_df),
            'liquidity_analysis': self._analyze_liquidity(etf_df),
            'cost_analysis': self._analyze_costs(etf_df),
            
            'macro_environment': macro_data,
            'data_quality_report': self._create_data_quality_report(etf_df),
            'provider_status': provider_status
        }
        
        logger.info("ĞšĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ¾Ñ‚Ñ‡ĞµÑ‚ ÑĞ¾Ğ·Ğ´Ğ°Ğ½")
        return report
    
    def _analyze_performance(self, df: pd.DataFrame) -> Dict:
        """ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ´Ğ¾Ñ…Ğ¾Ğ´Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ‘ĞŸĞ˜Ğ¤Ğ¾Ğ²"""
        valid_returns = df[df['annual_return'].notna()]
        
        if len(valid_returns) == 0:
            return {'note': 'Ğ”Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¾ Ğ´Ğ¾Ñ…Ğ¾Ğ´Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½ĞµĞ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹'}
        
        return {
            'average_return': valid_returns['annual_return'].mean(),
            'median_return': valid_returns['annual_return'].median(),
            'best_performer': {
                'ticker': valid_returns.loc[valid_returns['annual_return'].idxmax(), 'ticker'],
                'return': valid_returns['annual_return'].max(),
                'name': valid_returns.loc[valid_returns['annual_return'].idxmax(), 'name']
            },
            'worst_performer': {
                'ticker': valid_returns.loc[valid_returns['annual_return'].idxmin(), 'ticker'],
                'return': valid_returns['annual_return'].min(),
                'name': valid_returns.loc[valid_returns['annual_return'].idxmin(), 'name']
            },
            'positive_returns_count': (valid_returns['annual_return'] > 0).sum(),
            'negative_returns_count': (valid_returns['annual_return'] < 0).sum()
        }
    
    def _analyze_risk(self, df: pd.DataFrame) -> Dict:
        """ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ñ€Ğ¸ÑĞºĞ¾Ğ² Ğ‘ĞŸĞ˜Ğ¤Ğ¾Ğ²"""
        valid_volatility = df[df['volatility'].notna()]
        
        if len(valid_volatility) == 0:
            return {'note': 'Ğ”Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¾ Ğ²Ğ¾Ğ»Ğ°Ñ‚Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½ĞµĞ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹'}
        
        return {
            'average_volatility': valid_volatility['volatility'].mean(),
            'median_volatility': valid_volatility['volatility'].median(),
            'lowest_risk': {
                'ticker': valid_volatility.loc[valid_volatility['volatility'].idxmin(), 'ticker'],
                'volatility': valid_volatility['volatility'].min(),
                'name': valid_volatility.loc[valid_volatility['volatility'].idxmin(), 'name']
            },
            'highest_risk': {
                'ticker': valid_volatility.loc[valid_volatility['volatility'].idxmax(), 'ticker'],
                'volatility': valid_volatility['volatility'].max(),
                'name': valid_volatility.loc[valid_volatility['volatility'].idxmax(), 'name']
            }
        }
    
    def _analyze_liquidity(self, df: pd.DataFrame) -> Dict:
        """ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ»Ğ¸ĞºĞ²Ğ¸Ğ´Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ‘ĞŸĞ˜Ğ¤Ğ¾Ğ²"""
        valid_volume = df[df['daily_volume'].notna()]
        
        if len(valid_volume) == 0:
            return {'note': 'Ğ”Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¾Ğ± Ğ¾Ğ±ÑŠĞµĞ¼Ğ°Ñ… Ñ‚Ğ¾Ñ€Ğ³Ğ¾Ğ² Ğ½ĞµĞ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹'}
        
        return {
            'average_daily_volume': valid_volume['daily_volume'].mean(),
            'median_daily_volume': valid_volume['daily_volume'].median(),
            'most_liquid': {
                'ticker': valid_volume.loc[valid_volume['daily_volume'].idxmax(), 'ticker'],
                'volume': valid_volume['daily_volume'].max(),
                'name': valid_volume.loc[valid_volume['daily_volume'].idxmax(), 'name']
            },
            'least_liquid': {
                'ticker': valid_volume.loc[valid_volume['daily_volume'].idxmin(), 'ticker'],
                'volume': valid_volume['daily_volume'].min(),
                'name': valid_volume.loc[valid_volume['daily_volume'].idxmin(), 'name']
            }
        }
    
    def _analyze_costs(self, df: pd.DataFrame) -> Dict:
        """ĞĞ½Ğ°Ğ»Ğ¸Ğ· ĞºĞ¾Ğ¼Ğ¸ÑÑĞ¸Ğ¹ Ğ‘ĞŸĞ˜Ğ¤Ğ¾Ğ²"""
        valid_costs = df[df['expense_ratio'].notna()]
        
        if len(valid_costs) == 0:
            return {'note': 'Ğ”Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¾ ĞºĞ¾Ğ¼Ğ¸ÑÑĞ¸ÑÑ… Ğ½ĞµĞ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹'}
        
        return {
            'average_expense_ratio': valid_costs['expense_ratio'].mean(),
            'median_expense_ratio': valid_costs['expense_ratio'].median(),
            'lowest_cost': {
                'ticker': valid_costs.loc[valid_costs['expense_ratio'].idxmin(), 'ticker'],
                'expense_ratio': valid_costs['expense_ratio'].min(),
                'name': valid_costs.loc[valid_costs['expense_ratio'].idxmin(), 'name']
            },
            'highest_cost': {
                'ticker': valid_costs.loc[valid_costs['expense_ratio'].idxmax(), 'ticker'],
                'expense_ratio': valid_costs['expense_ratio'].max(),
                'name': valid_costs.loc[valid_costs['expense_ratio'].idxmax(), 'name']
            }
        }
    
    def _create_data_quality_report(self, df: pd.DataFrame) -> Dict:
        """Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ¾Ñ‚Ñ‡ĞµÑ‚Ğ° Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…"""
        return {
            'total_records': len(df),
            'records_with_price_data': df['current_price'].notna().sum(),
            'records_with_return_data': df['annual_return'].notna().sum(),
            'records_with_volume_data': df['daily_volume'].notna().sum(),
            'records_with_volatility_data': df['volatility'].notna().sum(),
            'average_quality_score': df['data_quality_score'].mean(),
            'data_sources_distribution': df['data_source'].value_counts().to_dict(),
            'fallback_usage': {
                'primary_source_usage': (df['fallback_level'] == 0).sum(),
                'secondary_source_usage': (df['fallback_level'] == 1).sum(),
                'fallback_only_usage': (df['fallback_level'] >= 2).sum()
            }
        }