"""
ETF Data Collector with Fallback - –∫–æ–ª–ª–µ–∫—Ç–æ—Ä –¥–∞–Ω–Ω—ã—Ö –æ —Ä–æ—Å—Å–∏–π—Å–∫–∏—Ö –ë–ü–ò–§–∞—Ö —Å fallback —Å–∏—Å—Ç–µ–º–æ–π
"""

import pandas as pd
from datetime import datetime
from typing import Dict, List, Optional
import time

from fallback_manager import DataProviderManager
from fallback_system import AllProvidersUnavailableError
from config import KNOWN_ETFS
from logger_config import logger, log_performance


class ETFDataCollectorWithFallback:
    """
    –ö–æ–ª–ª–µ–∫—Ç–æ—Ä –¥–∞–Ω–Ω—ã—Ö –æ —Ä–æ—Å—Å–∏–π—Å–∫–∏—Ö –ë–ü–ò–§–∞—Ö —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º fallback —Å–∏—Å—Ç–µ–º—ã
    –†–∞—Å—à–∏—Ä—è–µ—Ç —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–≥–æ BaseETFCollector
    """
    
    def __init__(self):
        self.fallback_manager = DataProviderManager()
        self.known_etfs = KNOWN_ETFS
        
        logger.info("ETF Data Collector —Å fallback —Å–∏—Å—Ç–µ–º–æ–π –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
        logger.info(f"–ò–∑–≤–µ—Å—Ç–Ω—ã—Ö –ë–ü–ò–§–æ–≤ –≤ –±–∞–∑–µ: {len(self.known_etfs)}")
    
    @log_performance
    def collect_all_etf_data(self) -> pd.DataFrame:
        """
        –°–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –ø–æ –≤—Å–µ–º –∏–∑–≤–µ—Å—Ç–Ω—ã–º —Ä–æ—Å—Å–∏–π—Å–∫–∏–º –ë–ü–ò–§–∞–º
        
        Returns:
            pd.DataFrame: –î–∞–Ω–Ω—ã–µ –ø–æ –≤—Å–µ–º –ë–ü–ò–§–∞–º —Å –º–µ—Ç–∞–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π
        """
        logger.info("–ù–∞—á–∏–Ω–∞–µ–º —Å–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –ø–æ –≤—Å–µ–º —Ä–æ—Å—Å–∏–π—Å–∫–∏–º –ë–ü–ò–§–∞–º")
        
        etf_data_list = []
        successful_collections = 0
        failed_collections = 0
        
        for ticker, metadata in self.known_etfs.items():
            logger.info(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ –ë–ü–ò–§–∞: {ticker} ({metadata.get('name', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')})")
            
            try:
                # –ü–æ–ª—É—á–∞–µ–º –¥–∞–Ω–Ω—ã–µ —Å fallback
                result = self.fallback_manager.get_etf_data_with_fallback(ticker)
                
                # –û–±—ä–µ–¥–∏–Ω—è–µ–º –ø–æ–ª—É—á–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
                etf_record = self._merge_data_with_metadata(ticker, result.data, metadata)
                
                # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ fallback
                etf_record.update({
                    'data_source': result.source,
                    'fallback_level': result.fallback_level,
                    'data_quality_score': result.quality_score,
                    'collection_timestamp': datetime.now().isoformat(),
                    'warnings': result.warnings
                })
                
                etf_data_list.append(etf_record)
                successful_collections += 1
                
                logger.info(f"‚úì {ticker}: –¥–∞–Ω–Ω—ã–µ –ø–æ–ª—É—á–µ–Ω—ã –∏–∑ {result.source} (–∫–∞—á–µ—Å—Ç–≤–æ: {result.quality_score:.2f})")
                
            except AllProvidersUnavailableError as e:
                logger.error(f"‚úó {ticker}: –≤—Å–µ –ø—Ä–æ–≤–∞–π–¥–µ—Ä—ã –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã - {e}")
                
                # –°–æ–∑–¥–∞–µ–º –∑–∞–ø–∏—Å—å —Å –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
                etf_record = self._create_fallback_record(ticker, metadata)
                etf_data_list.append(etf_record)
                failed_collections += 1
                
            except Exception as e:
                logger.error(f"‚úó {ticker}: –Ω–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ - {e}")
                
                etf_record = self._create_fallback_record(ticker, metadata)
                etf_data_list.append(etf_record)
                failed_collections += 1
            
            # –ù–µ–±–æ–ª—å—à–∞—è –ø–∞—É–∑–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
            time.sleep(0.5)
        
        # –°–æ–∑–¥–∞–µ–º DataFrame
        df = pd.DataFrame(etf_data_list)
        
        # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º —Ä—ã–Ω–æ—á–Ω—ã–µ –¥–æ–ª–∏
        df = self._calculate_market_shares(df)
        
        # –õ–æ–≥–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        logger.info(f"–°–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –∑–∞–≤–µ—Ä—à–µ–Ω:")
        logger.info(f"  ‚úì –£—Å–ø–µ—à–Ω–æ: {successful_collections}")
        logger.info(f"  ‚úó –ù–µ—É–¥–∞—á–Ω–æ: {failed_collections}")
        logger.info(f"  üìä –í—Å–µ–≥–æ –ë–ü–ò–§–æ–≤: {len(df)}")
        
        return df
    
    def collect_etf_data(self, ticker: str) -> Dict:
        """
        –°–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –ø–æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–º—É –ë–ü–ò–§—É
        
        Args:
            ticker: –¢–∏–∫–µ—Ä –ë–ü–ò–§–∞
            
        Returns:
            Dict: –î–∞–Ω–Ω—ã–µ –æ –ë–ü–ò–§–µ
        """
        logger.info(f"–°–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –ø–æ –ë–ü–ò–§—É: {ticker}")
        
        try:
            result = self.fallback_manager.get_etf_data_with_fallback(ticker)
            
            metadata = self.known_etfs.get(ticker, {})
            etf_data = self._merge_data_with_metadata(ticker, result.data, metadata)
            
            etf_data.update({
                'data_source': result.source,
                'fallback_level': result.fallback_level,
                'data_quality_score': result.quality_score,
                'collection_timestamp': datetime.now().isoformat()
            })
            
            logger.info(f"–î–∞–Ω–Ω—ã–µ –ø–æ {ticker} –ø–æ–ª—É—á–µ–Ω—ã –∏–∑ {result.source}")
            return etf_data
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –ø–æ {ticker}: {e}")
            return self._create_fallback_record(ticker, self.known_etfs.get(ticker, {}))
    
    def get_etf_list(self) -> List[Dict]:
        """
        –ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –ë–ü–ò–§–æ–≤
        
        Returns:
            List[Dict]: –°–ø–∏—Å–æ–∫ –ë–ü–ò–§–æ–≤ —Å –±–∞–∑–æ–≤–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π
        """
        logger.info("–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –ë–ü–ò–§–æ–≤")
        
        try:
            result = self.fallback_manager.get_etf_list_with_fallback()
            
            # –û–±–æ–≥–∞—â–∞–µ–º —Å–ø–∏—Å–æ–∫ –¥–∞–Ω–Ω—ã–º–∏ –∏–∑ –Ω–∞—à–µ–π –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π
            enriched_list = []
            for security in result.data['securities']:
                ticker = security.get('ticker')
                if ticker in self.known_etfs:
                    security.update(self.known_etfs[ticker])
                enriched_list.append(security)
            
            logger.info(f"–ü–æ–ª—É—á–µ–Ω —Å–ø–∏—Å–æ–∫ –∏–∑ {len(enriched_list)} –ë–ü–ò–§–æ–≤ –∏–∑ {result.source}")
            return enriched_list
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å–ø–∏—Å–∫–∞ –ë–ü–ò–§–æ–≤: {e}")
            
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Å–ø–∏—Å–æ–∫ –∏–∑ –Ω–∞—à–µ–π –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π
            fallback_list = []
            for ticker, metadata in self.known_etfs.items():
                fallback_list.append({
                    'ticker': ticker,
                    'name': metadata.get('name', ticker),
                    'management_company': metadata.get('management_company', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ'),
                    'category': metadata.get('category', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ'),
                    'source': 'fallback_knowledge_base'
                })
            
            logger.info(f"–í–æ–∑–≤—Ä–∞—â–µ–Ω fallback —Å–ø–∏—Å–æ–∫ –∏–∑ {len(fallback_list)} –ë–ü–ò–§–æ–≤")
            return fallback_list
    
    def get_macro_data(self) -> Dict:
        """
        –ü–æ–ª—É—á–µ–Ω–∏–µ –º–∞–∫—Ä–æ—ç–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö
        
        Returns:
            Dict: –ú–∞–∫—Ä–æ—ç–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–∏–µ –ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏
        """
        logger.info("–ü–æ–ª—É—á–µ–Ω–∏–µ –º–∞–∫—Ä–æ—ç–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö")
        
        try:
            result = self.fallback_manager.get_macro_data_with_fallback()
            
            macro_data = result.data.copy()
            macro_data.update({
                'data_source': result.source,
                'quality_score': result.quality_score,
                'collection_timestamp': datetime.now().isoformat()
            })
            
            logger.info(f"–ú–∞–∫—Ä–æ—ç–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ –ø–æ–ª—É—á–µ–Ω—ã –∏–∑ {result.source}")
            return macro_data
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –º–∞–∫—Ä–æ—ç–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö: {e}")
            return {
                'currency_rates': {},
                'key_rate': None,
                'error': str(e),
                'data_source': 'error',
                'collection_timestamp': datetime.now().isoformat()
            }
    
    def get_provider_status(self) -> Dict:
        """
        –ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç—É—Å–∞ –≤—Å–µ—Ö –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤ –¥–∞–Ω–Ω—ã—Ö
        
        Returns:
            Dict: –°—Ç–∞—Ç—É—Å –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤
        """
        return self.fallback_manager.get_provider_status()
    
    def _merge_data_with_metadata(self, ticker: str, api_data: Dict, metadata: Dict) -> Dict:
        """
        –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∏–∑ API —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
        """
        merged_data = {
            'ticker': ticker,
            'name': metadata.get('name', ticker),
            'management_company': metadata.get('management_company', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ'),
            'category': metadata.get('category', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ'),
            'tracking_index': metadata.get('tracking_index', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ'),
            'inception_year': metadata.get('inception_year'),
            'expense_ratio': metadata.get('expense_ratio'),
            'benchmark': metadata.get('benchmark')
        }
        
        # –î–æ–±–∞–≤–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ API
        merged_data.update(api_data)
        
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –∫–ª—é—á–µ–≤—ã–µ –ø–æ–ª—è
        merged_data['current_price'] = (
            api_data.get('current_price') or 
            api_data.get('last_price') or 
            api_data.get('regularMarketPrice')
        )
        
        merged_data['annual_return'] = (
            api_data.get('return_annualized') or 
            api_data.get('return_1y') or 
            api_data.get('return_period')
        )
        
        merged_data['volatility'] = api_data.get('volatility')
        merged_data['daily_volume'] = api_data.get('avg_daily_volume')
        merged_data['daily_value_rub'] = api_data.get('avg_daily_value_rub')
        
        return merged_data
    
    def _create_fallback_record(self, ticker: str, metadata: Dict) -> Dict:
        """
        –°–æ–∑–¥–∞–Ω–∏–µ –∑–∞–ø–∏—Å–∏ —Å –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏ –ø—Ä–∏ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ API
        """
        return {
            'ticker': ticker,
            'name': metadata.get('name', ticker),
            'management_company': metadata.get('management_company', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ'),
            'category': metadata.get('category', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ'),
            'tracking_index': metadata.get('tracking_index', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ'),
            'inception_year': metadata.get('inception_year'),
            'expense_ratio': metadata.get('expense_ratio'),
            'benchmark': metadata.get('benchmark'),
            'current_price': None,
            'annual_return': None,
            'volatility': None,
            'daily_volume': None,
            'daily_value_rub': None,
            'data_source': 'fallback_metadata',
            'fallback_level': 99,
            'data_quality_score': 0.1,
            'collection_timestamp': datetime.now().isoformat(),
            'warnings': ['–î–∞–Ω–Ω—ã–µ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã –∏–∑ –≤—Å–µ—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤'],
            'data_availability': 'metadata_only'
        }
    
    def _calculate_market_shares(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        –†–∞—Å—á–µ—Ç —Ä—ã–Ω–æ—á–Ω—ã—Ö –¥–æ–ª–µ–π —É–ø—Ä–∞–≤–ª—è—é—â–∏—Ö –∫–æ–º–ø–∞–Ω–∏–π
        """
        logger.info("–†–∞—Å—á–µ—Ç —Ä—ã–Ω–æ—á–Ω—ã—Ö –¥–æ–ª–µ–π —É–ø—Ä–∞–≤–ª—è—é—â–∏—Ö –∫–æ–º–ø–∞–Ω–∏–π")
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º –∑–∞–ø–∏—Å–∏ —Å –¥–æ—Å—Ç—É–ø–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏ –æ–± –æ–±—ä–µ–º–∞—Ö
        valid_data = df[
            (df['daily_value_rub'].notna()) & 
            (df['daily_value_rub'] > 0)
        ].copy()
        
        if len(valid_data) == 0:
            logger.warning("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –æ–± –æ–±—ä–µ–º–∞—Ö –¥–ª—è —Ä–∞—Å—á–µ—Ç–∞ —Ä—ã–Ω–æ—á–Ω—ã—Ö –¥–æ–ª–µ–π")
            df['market_share_percent'] = None
            return df
        
        # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –ø—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω—É—é —Ä—ã–Ω–æ—á–Ω—É—é –∫–∞–ø–∏—Ç–∞–ª–∏–∑–∞—Ü–∏—é
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –¥–Ω–µ–≤–Ω—ã–µ –æ–±—ä–µ–º—ã —Ç–æ—Ä–≥–æ–≤ –∫–∞–∫ –ø—Ä–æ–∫—Å–∏
        valid_data['estimated_market_cap'] = valid_data['daily_value_rub'] * 30  # –ü—Ä–∏–º–µ—Ä–Ω–∞—è –æ—Ü–µ–Ω–∫–∞
        
        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ —É–ø—Ä–∞–≤–ª—è—é—â–∏–º –∫–æ–º–ø–∞–Ω–∏—è–º
        uk_totals = valid_data.groupby('management_company')['estimated_market_cap'].sum()
        total_market = uk_totals.sum()
        
        if total_market > 0:
            uk_shares = (uk_totals / total_market * 100).round(1)
            
            # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –¥–æ–ª—è—Ö —Ä—ã–Ω–∫–∞
            df['market_share_percent'] = df['management_company'].map(uk_shares)
            
            logger.info("–†—ã–Ω–æ—á–Ω—ã–µ –¥–æ–ª–∏ —Ä–∞—Å—Å—á–∏—Ç–∞–Ω—ã:")
            for uk, share in uk_shares.items():
                logger.info(f"  {uk}: {share}%")
        else:
            df['market_share_percent'] = None
        
        return df
    
    @log_performance
    def create_comprehensive_report(self) -> Dict:
        """
        –°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–≥–æ –æ—Ç—á–µ—Ç–∞ –ø–æ —Ä–æ—Å—Å–∏–π—Å–∫–∏–º –ë–ü–ò–§–∞–º
        
        Returns:
            Dict: –ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –æ—Ç—á–µ—Ç
        """
        logger.info("–°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–≥–æ –æ—Ç—á–µ—Ç–∞ –ø–æ —Ä–æ—Å—Å–∏–π—Å–∫–∏–º –ë–ü–ò–§–∞–º")
        
        # –°–æ–±–∏—Ä–∞–µ–º –¥–∞–Ω–Ω—ã–µ
        etf_df = self.collect_all_etf_data()
        macro_data = self.get_macro_data()
        provider_status = self.get_provider_status()
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ
        report = {
            'report_metadata': {
                'creation_date': datetime.now().isoformat(),
                'total_etfs_analyzed': len(etf_df),
                'data_sources_used': etf_df['data_source'].value_counts().to_dict(),
                'average_data_quality': etf_df['data_quality_score'].mean()
            },
            
            'market_overview': {
                'total_etfs': len(etf_df),
                'management_companies': etf_df['management_company'].nunique(),
                'categories': etf_df['category'].value_counts().to_dict(),
                'market_shares': etf_df.groupby('management_company')['market_share_percent'].first().dropna().to_dict()
            },
            
            'performance_analysis': self._analyze_performance(etf_df),
            'risk_analysis': self._analyze_risk(etf_df),
            'liquidity_analysis': self._analyze_liquidity(etf_df),
            'cost_analysis': self._analyze_costs(etf_df),
            
            'macro_environment': macro_data,
            'data_quality_report': self._create_data_quality_report(etf_df),
            'provider_status': provider_status
        }
        
        logger.info("–ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –æ—Ç—á–µ—Ç —Å–æ–∑–¥–∞–Ω")
        return report
    
    def _analyze_performance(self, df: pd.DataFrame) -> Dict:
        """–ê–Ω–∞–ª–∏–∑ –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç–∏ –ë–ü–ò–§–æ–≤"""
        valid_returns = df[df['annual_return'].notna()]
        
        if len(valid_returns) == 0:
            return {'note': '–î–∞–Ω–Ω—ã–µ –æ –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç–∏ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã'}
        
        return {
            'average_return': valid_returns['annual_return'].mean(),
            'median_return': valid_returns['annual_return'].median(),
            'best_performer': {
                'ticker': valid_returns.loc[valid_returns['annual_return'].idxmax(), 'ticker'],
                'return': valid_returns['annual_return'].max(),
                'name': valid_returns.loc[valid_returns['annual_return'].idxmax(), 'name']
            },
            'worst_performer': {
                'ticker': valid_returns.loc[valid_returns['annual_return'].idxmin(), 'ticker'],
                'return': valid_returns['annual_return'].min(),
                'name': valid_returns.loc[valid_returns['annual_return'].idxmin(), 'name']
            },
            'positive_returns_count': (valid_returns['annual_return'] > 0).sum(),
            'negative_returns_count': (valid_returns['annual_return'] < 0).sum()
        }
    
    def _analyze_risk(self, df: pd.DataFrame) -> Dict:
        """–ê–Ω–∞–ª–∏–∑ —Ä–∏—Å–∫–æ–≤ –ë–ü–ò–§–æ–≤"""
        valid_volatility = df[df['volatility'].notna()]
        
        if len(valid_volatility) == 0:
            return {'note': '–î–∞–Ω–Ω—ã–µ –æ –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç–∏ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã'}
        
        return {
            'average_volatility': valid_volatility['volatility'].mean(),
            'median_volatility': valid_volatility['volatility'].median(),
            'lowest_risk': {
                'ticker': valid_volatility.loc[valid_volatility['volatility'].idxmin(), 'ticker'],
                'volatility': valid_volatility['volatility'].min(),
                'name': valid_volatility.loc[valid_volatility['volatility'].idxmin(), 'name']
            },
            'highest_risk': {
                'ticker': valid_volatility.loc[valid_volatility['volatility'].idxmax(), 'ticker'],
                'volatility': valid_volatility['volatility'].max(),
                'name': valid_volatility.loc[valid_volatility['volatility'].idxmax(), 'name']
            }
        }
    
    def _analyze_liquidity(self, df: pd.DataFrame) -> Dict:
        """–ê–Ω–∞–ª–∏–∑ –ª–∏–∫–≤–∏–¥–Ω–æ—Å—Ç–∏ –ë–ü–ò–§–æ–≤"""
        valid_volume = df[df['daily_volume'].notna()]
        
        if len(valid_volume) == 0:
            return {'note': '–î–∞–Ω–Ω—ã–µ –æ–± –æ–±—ä–µ–º–∞—Ö —Ç–æ—Ä–≥–æ–≤ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã'}
        
        return {
            'average_daily_volume': valid_volume['daily_volume'].mean(),
            'median_daily_volume': valid_volume['daily_volume'].median(),
            'most_liquid': {
                'ticker': valid_volume.loc[valid_volume['daily_volume'].idxmax(), 'ticker'],
                'volume': valid_volume['daily_volume'].max(),
                'name': valid_volume.loc[valid_volume['daily_volume'].idxmax(), 'name']
            },
            'least_liquid': {
                'ticker': valid_volume.loc[valid_volume['daily_volume'].idxmin(), 'ticker'],
                'volume': valid_volume['daily_volume'].min(),
                'name': valid_volume.loc[valid_volume['daily_volume'].idxmin(), 'name']
            }
        }
    
    def _analyze_costs(self, df: pd.DataFrame) -> Dict:
        """–ê–Ω–∞–ª–∏–∑ –∫–æ–º–∏—Å—Å–∏–π –ë–ü–ò–§–æ–≤"""
        valid_costs = df[df['expense_ratio'].notna()]
        
        if len(valid_costs) == 0:
            return {'note': '–î–∞–Ω–Ω—ã–µ –æ –∫–æ–º–∏—Å—Å–∏—è—Ö –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã'}
        
        return {
            'average_expense_ratio': valid_costs['expense_ratio'].mean(),
            'median_expense_ratio': valid_costs['expense_ratio'].median(),
            'lowest_cost': {
                'ticker': valid_costs.loc[valid_costs['expense_ratio'].idxmin(), 'ticker'],
                'expense_ratio': valid_costs['expense_ratio'].min(),
                'name': valid_costs.loc[valid_costs['expense_ratio'].idxmin(), 'name']
            },
            'highest_cost': {
                'ticker': valid_costs.loc[valid_costs['expense_ratio'].idxmax(), 'ticker'],
                'expense_ratio': valid_costs['expense_ratio'].max(),
                'name': valid_costs.loc[valid_costs['expense_ratio'].idxmax(), 'name']
            }
        }
    
    def _create_data_quality_report(self, df: pd.DataFrame) -> Dict:
        """–°–æ–∑–¥–∞–Ω–∏–µ –æ—Ç—á–µ—Ç–∞ –æ –∫–∞—á–µ—Å—Ç–≤–µ –¥–∞–Ω–Ω—ã—Ö"""
        return {
            'total_records': len(df),
            'records_with_price_data': df['current_price'].notna().sum(),
            'records_with_return_data': df['annual_return'].notna().sum(),
            'records_with_volume_data': df['daily_volume'].notna().sum(),
            'records_with_volatility_data': df['volatility'].notna().sum(),
            'average_quality_score': df['data_quality_score'].mean(),
            'data_sources_distribution': df['data_source'].value_counts().to_dict(),
            'fallback_usage': {
                'primary_source_usage': (df['fallback_level'] == 0).sum(),
                'secondary_source_usage': (df['fallback_level'] == 1).sum(),
                'fallback_only_usage': (df['fallback_level'] >= 2).sum()
            }
        }