#!/usr/bin/env python3
"""
–û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ —Ñ–æ–Ω–¥–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏–∑–≤–µ—Å—Ç–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –∏ –ø—Ä—è–º–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏ ID
–ë–æ–ª–µ–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –¥–ª—è investfunds.ru
"""

import requests
from bs4 import BeautifulSoup
import re
import pandas as pd
from typing import Dict, List, Optional, Tuple
import time
import json
from pathlib import Path
import logging
from concurrent.futures import ThreadPoolExecutor, as_completed

class PatternBasedFundDiscovery:
    """–û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ —Ñ–æ–Ω–¥–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤"""
    
    def __init__(self, max_workers: int = 3):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
        })
        self.base_url = "https://investfunds.ru"
        self.max_workers = max_workers
        self.logger = self._setup_logger()
        
        # –ò–∑–≤–µ—Å—Ç–Ω—ã–µ ID –∏ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
        self.known_mappings = {
            'LQDT': 5973,
            'AKMB': 6225,
            'AKGD': 10147,
        }
        
    def _setup_logger(self) -> logging.Logger:
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–≥–µ—Ä–∞"""
        logger = logging.getLogger('PatternFundDiscovery')
        logger.setLevel(logging.INFO)
        
        if not logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
            handler.setFormatter(formatter)
            logger.addHandler(handler)
            
        return logger
    
    def check_fund_id_for_ticker(self, fund_id: int, ticker: str) -> Optional[Dict]:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —Å–æ–¥–µ—Ä–∂–∏—Ç –ª–∏ —Å—Ç—Ä–∞–Ω–∏—Ü–∞ —Ñ–æ–Ω–¥–∞ —É–∫–∞–∑–∞–Ω–Ω—ã–π —Ç–∏–∫–µ—Ä"""
        
        try:
            url = f"{self.base_url}/funds/{fund_id}/"
            response = self.session.get(url, timeout=10)
            
            if response.status_code != 200:
                return None
            
            soup = BeautifulSoup(response.content, 'html.parser')
            page_text = soup.get_text().upper()
            
            # –†–∞–∑–ª–∏—á–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã –ø–æ–∏—Å–∫–∞ —Ç–∏–∫–µ—Ä–∞
            ticker_patterns = [
                ticker.upper(),
                f"–ë–ü–ò–§ {ticker.upper()}",
                f"ETF {ticker.upper()}",
                f"{ticker.upper()} ",
                f" {ticker.upper()}",
                f"({ticker.upper()})",
                f"[{ticker.upper()}]",
            ]
            
            ticker_found = any(pattern in page_text for pattern in ticker_patterns)
            
            if ticker_found:
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞–Ω–Ω—ã–µ
                title = soup.find('h1')
                fund_name = title.get_text(strip=True) if title else 'Unknown'
                
                # –ò—â–µ–º –°–ß–ê
                nav_value = self._extract_nav_from_page(page_text)
                unit_price = self._extract_unit_price_from_page(page_text)
                
                return {
                    'fund_id': fund_id,
                    'ticker': ticker,
                    'name': fund_name,
                    'nav': nav_value or 0,
                    'unit_price': unit_price or 0,
                    'url': url,
                    'page_contains_ticker': True
                }
            
            return None
            
        except Exception as e:
            if fund_id % 1000 == 0:  # –õ–æ–≥–∏—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –∫–∞–∂–¥—É—é 1000 –æ—à–∏–±–∫—É
                self.logger.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ ID {fund_id}: {e}")
            return None
    
    def _extract_nav_from_page(self, page_text: str) -> Optional[float]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –°–ß–ê —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
        
        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –ø–æ–∏—Å–∫–∞ –°–ß–ê
        nav_patterns = [
            r'–°–ß–ê[:\s]*([0-9\s,]+(?:\.\d+)?)',
            r'—á–∏—Å—Ç—ã—Ö –∞–∫—Ç–∏–≤–æ–≤[:\s]*([0-9\s,]+(?:\.\d+)?)',
            r'–∞–∫—Ç–∏–≤—ã[:\s]*([0-9\s,]+(?:\.\d+)?)',
            r'(\d{1,3}(?:\s\d{3})*(?:\s\d{3})*(?:\.\d+)?)',  # –ë–æ–ª—å—à–∏–µ —á–∏—Å–ª–∞ —Å –ø—Ä–æ–±–µ–ª–∞–º–∏
        ]
        
        for pattern in nav_patterns:
            matches = re.findall(pattern, page_text, re.IGNORECASE)
            for match in matches:
                try:
                    # –û—á–∏—â–∞–µ–º —á–∏—Å–ª–æ
                    cleaned = re.sub(r'[^\d.]', '', match.replace(' ', '').replace(',', ''))
                    if cleaned:
                        value = float(cleaned)
                        # –°–ß–ê –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –±–æ–ª—å—à–µ 10 –º–ª–Ω —Ä—É–±–ª–µ–π
                        if value > 10_000_000:
                            return value
                except ValueError:
                    continue
        
        return None
    
    def _extract_unit_price_from_page(self, page_text: str) -> Optional[float]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ü–µ–Ω—É –ø–∞—è —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
        
        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –ø–æ–∏—Å–∫–∞ —Ü–µ–Ω—ã –ø–∞—è
        price_patterns = [
            r'—Ü–µ–Ω–∞ –ø–∞—è[:\s]*([0-9.,]+)',
            r'—Å—Ç–æ–∏–º–æ—Å—Ç—å –ø–∞—è[:\s]*([0-9.,]+)',
            r'–ø–∞–π[:\s]*([0-9.,]+)',
        ]
        
        for pattern in price_patterns:
            matches = re.findall(pattern, page_text, re.IGNORECASE)
            for match in matches:
                try:
                    cleaned = re.sub(r'[^\d.]', '', match.replace(',', '.'))
                    if cleaned:
                        value = float(cleaned)
                        # –¶–µ–Ω–∞ –ø–∞—è –æ–±—ã—á–Ω–æ –æ—Ç 0.1 –¥–æ 10000 —Ä—É–±–ª–µ–π
                        if 0.1 <= value <= 10000:
                            return value
                except ValueError:
                    continue
        
        return None
    
    def scan_id_ranges_for_tickers(self, tickers: List[str], id_ranges: List[Tuple[int, int]]) -> Dict[str, Dict]:
        """–°–∫–∞–Ω–∏—Ä—É–µ—Ç –¥–∏–∞–ø–∞–∑–æ–Ω—ã ID –¥–ª—è –ø–æ–∏—Å–∫–∞ —Ç–∏–∫–µ—Ä–æ–≤"""
        
        results = {}
        total_checks = sum(end - start + 1 for start, end in id_ranges) * len(tickers)
        
        self.logger.info(f"üîç –°–∫–∞–Ω–∏—Ä—É–µ–º {len(tickers)} —Ç–∏–∫–µ—Ä–æ–≤ –≤ {len(id_ranges)} –¥–∏–∞–ø–∞–∑–æ–Ω–∞—Ö")
        self.logger.info(f"üìä –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–æ–≤–µ—Ä–æ–∫: {total_checks}")
        
        for ticker in tickers:
            if ticker in results:
                continue
                
            self.logger.info(f"\nüéØ –ò—â–µ–º {ticker}")
            
            found = False
            for start_id, end_id in id_ranges:
                if found:
                    break
                    
                self.logger.info(f"   –î–∏–∞–ø–∞–∑–æ–Ω {start_id}-{end_id}")
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–∏–∞–ø–∞–∑–æ–Ω —Å –º–Ω–æ–≥–æ–ø–æ—Ç–æ—á–Ω–æ—Å—Ç—å—é
                with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
                    futures = {
                        executor.submit(self.check_fund_id_for_ticker, fund_id, ticker): fund_id
                        for fund_id in range(start_id, end_id + 1)
                    }
                    
                    for future in as_completed(futures):
                        result = future.result()
                        if result:
                            results[ticker] = result
                            found = True
                            self.logger.info(f"‚úÖ {ticker} -> ID {result['fund_id']} (–°–ß–ê: {result['nav']/1e9:.1f} –º–ª—Ä–¥ ‚ÇΩ)")
                            break
            
            if not found:
                self.logger.warning(f"‚ùå {ticker}: –Ω–µ –Ω–∞–π–¥–µ–Ω")
        
        return results
    
    def intelligent_search(self, etf_data: pd.DataFrame, max_funds: int = 50) -> Dict[str, Dict]:
        """–£–º–Ω—ã–π –ø–æ–∏—Å–∫ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∏–∑–≤–µ—Å—Ç–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤"""
        
        # –ù–∞—á–∏–Ω–∞–µ–º —Å –∏–∑–≤–µ—Å—Ç–Ω—ã—Ö –º–∞–ø–ø–∏–Ω–≥–æ–≤
        results = {}
        
        # –î–æ–±–∞–≤–ª—è–µ–º —É–∂–µ –∏–∑–≤–µ—Å—Ç–Ω—ã–µ —Ñ–æ–Ω–¥—ã
        self.logger.info("üìã –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏–∑–≤–µ—Å—Ç–Ω—ã–µ –º–∞–ø–ø–∏–Ω–≥–∏")
        for ticker, fund_id in self.known_mappings.items():
            if ticker in etf_data['ticker'].values:
                try:
                    from investfunds_parser import InvestFundsParser
                    parser = InvestFundsParser()
                    fund_data = parser.get_fund_data(fund_id)
                    
                    if fund_data and fund_data.get('nav', 0) > 0:
                        results[ticker] = {
                            'fund_id': fund_id,
                            'ticker': ticker,
                            'nav': fund_data['nav'],
                            'unit_price': fund_data['unit_price'],
                            'name': fund_data['name'],
                            'verified': True
                        }
                        self.logger.info(f"‚úÖ {ticker} (–∏–∑–≤–µ—Å—Ç–Ω—ã–π): –°–ß–ê {fund_data['nav']/1e9:.1f} –º–ª—Ä–¥ ‚ÇΩ")
                except Exception as e:
                    self.logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ –∏–∑–≤–µ—Å—Ç–Ω–æ–≥–æ {ticker}: {e}")
        
        # –ò—â–µ–º –æ—Å—Ç–∞–ª—å–Ω—ã–µ —Ñ–æ–Ω–¥—ã
        remaining_tickers = [
            ticker for ticker in etf_data['ticker'].tolist()[:max_funds]
            if ticker not in results
        ]
        
        if not remaining_tickers:
            return results
        
        self.logger.info(f"üîç –ò—â–µ–º {len(remaining_tickers)} –Ω–æ–≤—ã—Ö —Ñ–æ–Ω–¥–æ–≤")
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤–Ω—ã–µ –¥–∏–∞–ø–∞–∑–æ–Ω—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏–∑–≤–µ—Å—Ç–Ω—ã—Ö ID
        known_ids = list(self.known_mappings.values())
        if known_ids:
            min_known = min(known_ids)
            max_known = max(known_ids)
            
            # –£–º–Ω—ã–µ –¥–∏–∞–ø–∞–∑–æ–Ω—ã –≤–æ–∫—Ä—É–≥ –∏–∑–≤–µ—Å—Ç–Ω—ã—Ö ID
            search_ranges = [
                (max_known + 1, min(max_known + 1000, 15000)),    # –ü–æ—Å–ª–µ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ
                (max(1, min_known - 1000), min_known - 1),        # –î–æ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–≥–æ
                (min_known + 1, max_known - 1),                   # –ú–µ–∂–¥—É –∏–∑–≤–µ—Å—Ç–Ω—ã–º–∏
                (1, 3000),                                        # –†–∞–Ω–Ω–∏–µ —Ñ–æ–Ω–¥—ã
                (8000, 12000),                                    # –°—Ä–µ–¥–Ω–∏–π –¥–∏–∞–ø–∞–∑–æ–Ω
            ]
        else:
            # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –¥–∏–∞–ø–∞–∑–æ–Ω—ã
            search_ranges = [
                (1, 3000),
                (5000, 8000),
                (10000, 13000),
            ]
        
        # –ü–æ–∏—Å–∫ –≤ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã—Ö –¥–∏–∞–ø–∞–∑–æ–Ω–∞—Ö
        new_results = self.scan_id_ranges_for_tickers(remaining_tickers, search_ranges)
        results.update(new_results)
        
        return results
    
    def create_expanded_mapping(self, results: Dict[str, Dict]) -> Dict[str, int]:
        """–°–æ–∑–¥–∞–µ—Ç —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –º–∞–ø–ø–∏–Ω–≥ –¥–ª—è investfunds_parser.py"""
        
        mapping = {}
        for ticker, data in results.items():
            if 'fund_id' in data and data.get('nav', 0) > 0:
                mapping[ticker] = data['fund_id']
        
        return mapping
    
    def update_investfunds_parser(self, new_mappings: Dict[str, int]):
        """–û–±–Ω–æ–≤–ª—è–µ—Ç –º–∞–ø–ø–∏–Ω–≥ –≤ investfunds_parser.py"""
        
        try:
            parser_file = Path("investfunds_parser.py")
            if not parser_file.exists():
                self.logger.error("‚ùå –§–∞–π–ª investfunds_parser.py –Ω–µ –Ω–∞–π–¥–µ–Ω")
                return
            
            # –ß–∏—Ç–∞–µ–º —Ñ–∞–π–ª
            with open(parser_file, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # –ò—â–µ–º –º–∞–ø–ø–∏–Ω–≥
            mapping_start = content.find("self.fund_mapping = {")
            mapping_end = content.find("}", mapping_start) + 1
            
            if mapping_start == -1:
                self.logger.error("‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω fund_mapping –≤ —Ñ–∞–π–ª–µ")
                return
            
            # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π –º–∞–ø–ø–∏–Ω–≥
            mapping_lines = ["        # –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –º–∞–ø–ø–∏–Ω–≥ —Ç–∏–∫–µ—Ä–æ–≤ –Ω–∞ ID —Ñ–æ–Ω–¥–æ–≤"]
            mapping_lines.append("        self.fund_mapping = {")
            
            for ticker, fund_id in sorted(new_mappings.items()):
                mapping_lines.append(f"            '{ticker}': {fund_id},  # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –Ω–∞–π–¥–µ–Ω")
            
            mapping_lines.append("        }")
            
            # –°–æ–∑–¥–∞–µ–º –±—ç–∫–∞–ø
            backup_file = f"investfunds_parser_backup_{pd.Timestamp.now().strftime('%Y%m%d_%H%M%S')}.py"
            with open(backup_file, 'w', encoding='utf-8') as f:
                f.write(content)
            
            # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
            new_content = (
                content[:mapping_start] + 
                "\n".join(mapping_lines) + 
                content[mapping_end:]
            )
            
            with open(parser_file, 'w', encoding='utf-8') as f:
                f.write(new_content)
            
            self.logger.info(f"‚úÖ –û–±–Ω–æ–≤–ª–µ–Ω –º–∞–ø–ø–∏–Ω–≥: {len(new_mappings)} —Ñ–æ–Ω–¥–æ–≤")
            self.logger.info(f"üìÑ –ë—ç–∫–∞–ø: {backup_file}")
            
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è: {e}")

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
    data_files = list(Path('.').glob('enhanced_etf_data_*.csv'))
    if not data_files:
        data_files = list(Path('.').glob('full_moex_etf_data_*.csv'))
    
    if not data_files:
        print("‚ùå –§–∞–π–ª—ã —Å –¥–∞–Ω–Ω—ã–º–∏ ETF –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
        return
    
    latest_data = max(data_files, key=lambda x: x.stat().st_mtime)
    etf_data = pd.read_csv(latest_data)
    
    print(f"üìä –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(etf_data)} ETF")
    
    # –°–æ–∑–¥–∞–µ–º –ø–æ–∏—Å–∫–æ–≤–∏–∫
    discoverer = PatternBasedFundDiscovery(max_workers=3)
    
    print(f"\nüöÄ –ó–∞–ø—É—Å–∫–∞–µ–º —É–º–Ω—ã–π –ø–æ–∏—Å–∫ (–ø–µ—Ä–≤—ã–µ 30 —Ñ–æ–Ω–¥–æ–≤)")
    
    # –£–º–Ω—ã–π –ø–æ–∏—Å–∫
    results = discoverer.intelligent_search(etf_data, max_funds=30)
    
    # –°–æ–∑–¥–∞–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –º–∞–ø–ø–∏–Ω–≥
    new_mappings = discoverer.create_expanded_mapping(results)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    save_data = {
        'timestamp': pd.Timestamp.now().isoformat(),
        'total_found': len(results),
        'mappings': new_mappings,
        'detailed_results': {k: v for k, v in results.items() if 'nav' in v}
    }
    
    with open('pattern_discovery_results.json', 'w', encoding='utf-8') as f:
        json.dump(save_data, f, indent=2, ensure_ascii=False, default=str)
    
    print(f"\nüéâ –†–ï–ó–£–õ–¨–¢–ê–¢–´ –£–ú–ù–û–ì–û –ü–û–ò–°–ö–ê:")
    print(f"   ‚úÖ –ù–∞–π–¥–µ–Ω–æ: {len(results)} —Ñ–æ–Ω–¥–æ–≤")
    print(f"   üìà –ü–æ–∫—Ä—ã—Ç–∏–µ: {len(results)/30*100:.1f}%")
    
    if new_mappings:
        print(f"\nüèÜ –ù–∞–π–¥–µ–Ω–Ω—ã–µ –º–∞–ø–ø–∏–Ω–≥–∏:")
        for ticker, fund_id in sorted(new_mappings.items()):
            nav = results[ticker].get('nav', 0)
            print(f"   {ticker} -> ID {fund_id} (–°–ß–ê: {nav/1e9:.1f} –º–ª—Ä–¥ ‚ÇΩ)")
        
        # –û–±–Ω–æ–≤–ª—è–µ–º parser
        discoverer.update_investfunds_parser(new_mappings)
    
    print(f"\nüíæ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ pattern_discovery_results.json")

if __name__ == "__main__":
    main()