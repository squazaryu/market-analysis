#!/usr/bin/env python3
"""
–ü–æ–∏—Å–∫ —Ñ–æ–Ω–¥–æ–≤ –ø–æ —Ç–∏–∫–µ—Ä–∞–º –Ω–∞ investfunds.ru —á–µ—Ä–µ–∑ –ø–æ–∏—Å–∫–æ–≤—É—é —Å–∏—Å—Ç–µ–º—É —Å–∞–π—Ç–∞
–ë–æ–ª–µ–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –≤–º–µ—Å—Ç–æ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –≤—Å–µ—Ö ID
"""

import requests
from bs4 import BeautifulSoup
import re
import pandas as pd
from typing import Dict, List, Optional
import time
import json
from pathlib import Path
import logging

class TickerBasedFundFinder:
    """–ü–æ–∏—Å–∫ —Ñ–æ–Ω–¥–æ–≤ –ø–æ —Ç–∏–∫–µ—Ä–∞–º —á–µ—Ä–µ–∑ –ø–æ–∏—Å–∫–æ–≤—É—é —Å–∏—Å—Ç–µ–º—É investfunds.ru"""
    
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
        })
        self.base_url = "https://investfunds.ru"
        self.logger = self._setup_logger()
        self.found_mappings = {}
        
    def _setup_logger(self) -> logging.Logger:
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–≥–µ—Ä–∞"""
        logger = logging.getLogger('TickerFundFinder')
        logger.setLevel(logging.INFO)
        
        if not logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
            handler.setFormatter(formatter)
            logger.addHandler(handler)
            
        return logger
    
    def search_fund_by_ticker(self, ticker: str) -> List[Dict]:
        """–ü–æ–∏—Å–∫ —Ñ–æ–Ω–¥–∞ –ø–æ —Ç–∏–∫–µ—Ä—É —á–µ—Ä–µ–∑ –ø–æ–∏—Å–∫–æ–≤—É—é —Å–∏—Å—Ç–µ–º—É"""
        
        search_results = []
        
        try:
            # –†–∞–∑–ª–∏—á–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã –ø–æ–∏—Å–∫–∞
            search_queries = [
                ticker,
                f"–ë–ü–ò–§ {ticker}",
                f"ETF {ticker}",
                f"—Ñ–æ–Ω–¥ {ticker}"
            ]
            
            for query in search_queries:
                self.logger.info(f"üîç –ò—â–µ–º '{query}' –Ω–∞ investfunds.ru")
                
                # –ü–æ–∏—Å–∫ —á–µ—Ä–µ–∑ –æ—Å–Ω–æ–≤–Ω—É—é –ø–æ–∏—Å–∫–æ–≤—É—é —Å—Ç—Ä–æ–∫—É
                search_url = f"{self.base_url}/search/"
                
                # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º POST –∑–∞–ø—Ä–æ—Å –∫–∞–∫ –¥–µ–ª–∞–µ—Ç —Å–∞–π—Ç
                search_data = {
                    'q': query,
                    'submit': '–ü–æ–∏—Å–∫'
                }
                
                response = self.session.get(f"{search_url}?q={query}", timeout=15)
                
                if response.status_code != 200:
                    self.logger.warning(f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ '{query}': —Å—Ç–∞—Ç—É—Å {response.status_code}")
                    continue
                
                soup = BeautifulSoup(response.content, 'html.parser')
                
                # –ò—â–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞
                results = self._extract_search_results(soup, ticker, query)
                search_results.extend(results)
                
                time.sleep(1)  # –ü–∞—É–∑–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
                
                if results:
                    break  # –ï—Å–ª–∏ –Ω–∞—à–ª–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã, –Ω–µ –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º –ø–æ–∏—Å–∫
            
            # –î–µ–¥—É–ø–ª–∏—Ü–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            unique_results = self._deduplicate_results(search_results)
            
            return unique_results
            
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ {ticker}: {e}")
            return []
    
    def _extract_search_results(self, soup: BeautifulSoup, ticker: str, query: str) -> List[Dict]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞ —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
        
        results = []
        
        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –ø–æ–∏—Å–∫–∞ —Å—Å—ã–ª–æ–∫ –Ω–∞ —Ñ–æ–Ω–¥—ã
        fund_link_patterns = [
            r'/funds/(\d+)/',
            r'/fund/(\d+)/',
            r'/pif/(\d+)/'
        ]
        
        # –ò—â–µ–º –≤—Å–µ —Å—Å—ã–ª–∫–∏
        all_links = soup.find_all('a', href=True)
        
        for link in all_links:
            href = link.get('href', '')
            link_text = link.get_text(strip=True)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —ç—Ç–æ —Å—Å—ã–ª–∫–æ–π –Ω–∞ —Ñ–æ–Ω–¥
            for pattern in fund_link_patterns:
                match = re.search(pattern, href)
                if match:
                    fund_id = int(match.group(1))
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å
                    relevance_score = self._calculate_relevance(link_text, ticker, query)
                    
                    if relevance_score > 0.1:  # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
                        results.append({
                            'fund_id': fund_id,
                            'name': link_text,
                            'url': f"{self.base_url}{href}" if not href.startswith('http') else href,
                            'relevance': relevance_score,
                            'search_query': query
                        })
        
        # –¢–∞–∫–∂–µ –∏—â–µ–º –≤ —Ç–µ–∫—Å—Ç–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã —É–ø–æ–º–∏–Ω–∞–Ω–∏—è —Ç–∏–∫–µ—Ä–∞ —Å ID
        page_text = soup.get_text()
        ticker_mentions = self._find_ticker_with_ids(page_text, ticker)
        
        for mention in ticker_mentions:
            if not any(r['fund_id'] == mention['fund_id'] for r in results):
                results.append(mention)
        
        self.logger.info(f"   –ù–∞–π–¥–µ–Ω–æ {len(results)} –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã—Ö —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π –¥–ª—è '{query}'")
        
        return results
    
    def _calculate_relevance(self, text: str, ticker: str, query: str) -> float:
        """–í—ã—á–∏—Å–ª—è–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –Ω–∞–π–¥–µ–Ω–Ω–æ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞"""
        
        text_upper = text.upper()
        ticker_upper = ticker.upper()
        
        relevance = 0.0
        
        # –ü—Ä—è–º–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ —Ç–∏–∫–µ—Ä–∞
        if ticker_upper in text_upper:
            relevance += 0.8
        
        # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
        keywords = ['–ë–ü–ò–§', 'ETF', '—Ñ–æ–Ω–¥', '–∏–Ω–¥–µ–∫—Å', ticker_upper]
        for keyword in keywords:
            if keyword.upper() in text_upper:
                relevance += 0.1
        
        # –®—Ç—Ä–∞—Ñ –∑–∞ –Ω–µ—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Å–ª–æ–≤–∞
        irrelevant_words = ['–Ω–æ–≤–æ—Å—Ç–∏', '—Å—Ç–∞—Ç—å—è', '–∞–Ω–∞–ª–∏—Ç–∏–∫–∞', '–±–ª–æ–≥']
        for word in irrelevant_words:
            if word.upper() in text_upper:
                relevance -= 0.3
        
        return max(0.0, min(1.0, relevance))
    
    def _find_ticker_with_ids(self, text: str, ticker: str) -> List[Dict]:
        """–ò—â–µ—Ç —É–ø–æ–º–∏–Ω–∞–Ω–∏—è —Ç–∏–∫–µ—Ä–∞ —Ä—è–¥–æ–º —Å ID —Ñ–æ–Ω–¥–æ–≤ –≤ —Ç–µ–∫—Å—Ç–µ"""
        
        results = []
        
        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –ø–æ–∏—Å–∫–∞ "—Ç–∏–∫–µ—Ä + ID"
        patterns = [
            rf'{ticker}\s*.*?/funds/(\d+)',
            rf'/funds/(\d+)\s*.*?{ticker}',
            rf'{ticker}.*?(\d{{4,6}})',  # –¢–∏–∫–µ—Ä + 4-6 —Ü–∏—Ñ—Ä (–≤–æ–∑–º–æ–∂–Ω—ã–π ID)
        ]
        
        for pattern in patterns:
            matches = re.finditer(pattern, text, re.IGNORECASE | re.DOTALL)
            for match in matches:
                try:
                    fund_id = int(match.group(1))
                    if 1 <= fund_id <= 50000:  # –†–∞–∑—É–º–Ω—ã–π –¥–∏–∞–ø–∞–∑–æ–Ω ID
                        results.append({
                            'fund_id': fund_id,
                            'name': f'–ù–∞–π–¥–µ–Ω –ø–æ –ø–∞—Ç—Ç–µ—Ä–Ω—É: {ticker}',
                            'url': f"{self.base_url}/funds/{fund_id}/",
                            'relevance': 0.7,
                            'search_query': f'pattern:{ticker}'
                        })
                except ValueError:
                    continue
        
        return results
    
    def _deduplicate_results(self, results: List[Dict]) -> List[Dict]:
        """–£–¥–∞–ª—è–µ—Ç –¥—É–±–ª–∏–∫–∞—Ç—ã –∏ —Å–æ—Ä—Ç–∏—Ä—É–µ—Ç –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏"""
        
        seen_ids = set()
        unique_results = []
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
        sorted_results = sorted(results, key=lambda x: x['relevance'], reverse=True)
        
        for result in sorted_results:
            if result['fund_id'] not in seen_ids:
                seen_ids.add(result['fund_id'])
                unique_results.append(result)
        
        return unique_results
    
    def verify_and_get_nav_data(self, fund_id: int, ticker: str) -> Optional[Dict]:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –Ω–∞–π–¥–µ–Ω–Ω—ã–π —Ñ–æ–Ω–¥ –∏ –ø–æ–ª—É—á–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –°–ß–ê"""
        
        try:
            from investfunds_parser import InvestFundsParser
            parser = InvestFundsParser()
            
            # –ü–æ–ª—É—á–∞–µ–º –¥–∞–Ω–Ω—ã–µ —Ñ–æ–Ω–¥–∞
            fund_data = parser.get_fund_data(fund_id, use_cache=False)
            
            if fund_data and fund_data.get('nav', 0) > 0:
                # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ - —Å–æ–¥–µ—Ä–∂–∏—Ç –ª–∏ —Å—Ç—Ä–∞–Ω–∏—Ü–∞ —Ç–∏–∫–µ—Ä
                url = f"{self.base_url}/funds/{fund_id}/"
                response = self.session.get(url, timeout=10)
                
                if response.status_code == 200:
                    page_text = response.text.upper()
                    ticker_found = ticker.upper() in page_text
                    
                    # –ò—â–µ–º —Ç–∞–∫–∂–µ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã –Ω–∞–ø–∏—Å–∞–Ω–∏—è
                    alt_variations = [
                        f"–ë–ü–ò–§ {ticker}",
                        f"ETF {ticker}",
                        f"{ticker} ",
                        f" {ticker}",
                    ]
                    
                    for variation in alt_variations:
                        if variation.upper() in page_text:
                            ticker_found = True
                            break
                    
                    if ticker_found:
                        self.logger.info(f"‚úÖ –ü–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–æ: {ticker} -> ID {fund_id} (–°–ß–ê: {fund_data['nav']/1e9:.1f} –º–ª—Ä–¥ ‚ÇΩ)")
                        return {
                            'ticker': ticker,
                            'fund_id': fund_id,
                            'nav': fund_data['nav'],
                            'unit_price': fund_data['unit_price'],
                            'name': fund_data['name'],
                            'verified': True,
                            'url': url
                        }
                    else:
                        self.logger.warning(f"‚ö†Ô∏è ID {fund_id} –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç —Ç–∏–∫–µ—Ä {ticker}")
                        return None
                else:
                    self.logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ–≤–µ—Ä–∏—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—É ID {fund_id}")
                    return None
            else:
                self.logger.warning(f"‚ö†Ô∏è ID {fund_id} –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –¥–∞–Ω–Ω—ã—Ö –°–ß–ê")
                return None
                
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ ID {fund_id} –¥–ª—è {ticker}: {e}")
            return None
    
    def find_all_funds_by_tickers(self, etf_data: pd.DataFrame, max_funds: int = None) -> Dict[str, Dict]:
        """–ò—â–µ—Ç –≤—Å–µ —Ñ–æ–Ω–¥—ã –ø–æ –∏—Ö —Ç–∏–∫–µ—Ä–∞–º"""
        
        self.logger.info(f"üéØ –ù–∞—á–∏–Ω–∞–µ–º –ø–æ–∏—Å–∫ –ø–æ —Ç–∏–∫–µ—Ä–∞–º –¥–ª—è {len(etf_data)} —Ñ–æ–Ω–¥–æ–≤")
        
        results = {}
        processed_count = 0
        
        for idx, row in etf_data.iterrows():
            if max_funds and processed_count >= max_funds:
                break
                
            ticker = row['ticker']
            processed_count += 1
            
            self.logger.info(f"\nüîç [{processed_count}/{len(etf_data)}] –ò—â–µ–º {ticker}")
            
            # –ü–æ–∏—Å–∫ –≤–æ–∑–º–æ–∂–Ω—ã—Ö —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π
            search_results = self.search_fund_by_ticker(ticker)
            
            if not search_results:
                self.logger.warning(f"‚ùå {ticker}: –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –ø–æ–∏—Å–∫–µ")
                continue
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–π–¥–µ–Ω–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ —É–±—ã–≤–∞–Ω–∏—é —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
            verified_data = None
            
            for result in search_results[:3]:  # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–æ–ø-3 —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
                fund_id = result['fund_id']
                
                self.logger.info(f"   üß™ –ü—Ä–æ–≤–µ—Ä—è–µ–º ID {fund_id} (—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å: {result['relevance']:.2f})")
                
                verified_data = self.verify_and_get_nav_data(fund_id, ticker)
                
                if verified_data:
                    results[ticker] = verified_data
                    self.found_mappings[ticker] = fund_id
                    break
                
                time.sleep(0.5)  # –ü–∞—É–∑–∞ –º–µ–∂–¥—É –ø—Ä–æ–≤–µ—Ä–∫–∞–º–∏
            
            if not verified_data:
                self.logger.warning(f"‚ùå {ticker}: –Ω–∞–π–¥–µ–Ω—ã —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è, –Ω–æ –Ω–µ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω—ã")
            
            time.sleep(2)  # –ü–∞—É–∑–∞ –º–µ–∂–¥—É –ø–æ–∏—Å–∫–∞–º–∏ —Ä–∞–∑–Ω—ã—Ö —Ç–∏–∫–µ—Ä–æ–≤
        
        self.logger.info(f"\nüéâ –ü–æ–∏—Å–∫ –∑–∞–≤–µ—Ä—à–µ–Ω:")
        self.logger.info(f"   ‚úÖ –ù–∞–π–¥–µ–Ω–æ –∏ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–æ: {len(results)} —Ñ–æ–Ω–¥–æ–≤")
        self.logger.info(f"   üìä –ü–æ–∫—Ä—ã—Ç–∏–µ: {len(results)/len(etf_data)*100:.1f}%")
        
        return results
    
    def save_results(self, results: Dict, filename: str = "ticker_search_results.json"):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞"""
        
        save_data = {
            'timestamp': pd.Timestamp.now().isoformat(),
            'total_found': len(results),
            'fund_mappings': self.found_mappings,
            'detailed_results': results
        }
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(save_data, f, indent=2, ensure_ascii=False, default=str)
        
        self.logger.info(f"üíæ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {filename}")
    
    def update_investfunds_parser_mapping(self, results: Dict):
        """–û–±–Ω–æ–≤–ª—è–µ—Ç –º–∞–ø–ø–∏–Ω–≥ –≤ investfunds_parser.py"""
        
        try:
            # –ß–∏—Ç–∞–µ–º —Ç–µ–∫—É—â–∏–π —Ñ–∞–π–ª
            parser_file = Path("investfunds_parser.py")
            if not parser_file.exists():
                self.logger.error("‚ùå –§–∞–π–ª investfunds_parser.py –Ω–µ –Ω–∞–π–¥–µ–Ω")
                return
            
            with open(parser_file, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # –ò—â–µ–º —Å–µ–∫—Ü–∏—é —Å –º–∞–ø–ø–∏–Ω–≥–æ–º
            mapping_start = content.find("self.fund_mapping = {")
            mapping_end = content.find("}", mapping_start) + 1
            
            if mapping_start == -1 or mapping_end == -1:
                self.logger.error("‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–∞ —Å–µ–∫—Ü–∏—è fund_mapping –≤ —Ñ–∞–π–ª–µ")
                return
            
            # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π –º–∞–ø–ø–∏–Ω–≥, –æ–±—ä–µ–¥–∏–Ω—è—è —Å—Ç–∞—Ä—ã–π –∏ –Ω–æ–≤—ã–π
            new_mapping_lines = []
            
            # –î–æ–±–∞–≤–ª—è–µ–º –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π
            new_mapping_lines.append("        # –ú–∞–ø–ø–∏–Ω–≥ —Ç–∏–∫–µ—Ä–æ–≤ –Ω–∞ ID —Ñ–æ–Ω–¥–æ–≤ (–∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–æ)")
            new_mapping_lines.append("        self.fund_mapping = {")
            
            # –î–æ–±–∞–≤–ª—è–µ–º –Ω–∞–π–¥–µ–Ω–Ω—ã–µ –º–∞–ø–ø–∏–Ω–≥–∏
            for ticker, data in results.items():
                fund_id = data['fund_id']
                comment = f"  # {data['name'][:50]}..." if len(data['name']) > 50 else f"  # {data['name']}"
                new_mapping_lines.append(f"            '{ticker}': {fund_id},{comment}")
            
            new_mapping_lines.append("        }")
            
            # –ó–∞–º–µ–Ω—è–µ–º —Å—Ç–∞—Ä—ã–π –º–∞–ø–ø–∏–Ω–≥ –Ω–æ–≤—ã–º
            new_content = (
                content[:mapping_start] + 
                "\n".join(new_mapping_lines) + 
                content[mapping_end:]
            )
            
            # –°–æ–∑–¥–∞–µ–º –±—ç–∫–∞–ø
            backup_file = f"investfunds_parser_backup_{pd.Timestamp.now().strftime('%Y%m%d_%H%M%S')}.py"
            with open(backup_file, 'w', encoding='utf-8') as f:
                f.write(content)
            
            # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
            with open(parser_file, 'w', encoding='utf-8') as f:
                f.write(new_content)
            
            self.logger.info(f"‚úÖ –û–±–Ω–æ–≤–ª–µ–Ω –º–∞–ø–ø–∏–Ω–≥ –≤ investfunds_parser.py ({len(results)} —Ñ–æ–Ω–¥–æ–≤)")
            self.logger.info(f"üìÑ –ë—ç–∫–∞–ø —Å–æ—Ö—Ä–∞–Ω–µ–Ω –∫–∞–∫ {backup_file}")
            
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –º–∞–ø–ø–∏–Ω–≥–∞: {e}")

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ ETF
    data_files = list(Path('.').glob('enhanced_etf_data_*.csv'))
    if not data_files:
        data_files = list(Path('.').glob('full_moex_etf_data_*.csv'))
    
    if not data_files:
        print("‚ùå –§–∞–π–ª—ã —Å –¥–∞–Ω–Ω—ã–º–∏ ETF –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
        return
    
    latest_data = max(data_files, key=lambda x: x.stat().st_mtime)
    etf_data = pd.read_csv(latest_data)
    
    print(f"üìä –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(etf_data)} ETF –¥–ª—è –ø–æ–∏—Å–∫–∞ –ø–æ —Ç–∏–∫–µ—Ä–∞–º")
    
    # –°–æ–∑–¥–∞–µ–º –ø–æ–∏—Å–∫–æ–≤–∏–∫
    finder = TickerBasedFundFinder()
    
    print(f"\nüöÄ –ó–∞–ø—É—Å–∫–∞–µ–º –ø–æ–∏—Å–∫ –ø–æ —Ç–∏–∫–µ—Ä–∞–º (—Ç–µ—Å—Ç–∏—Ä—É–µ–º –ø–µ—Ä–≤—ã–µ 30 —Ñ–æ–Ω–¥–æ–≤)")
    
    # –ó–∞–ø—É—Å–∫–∞–µ–º –ø–æ–∏—Å–∫
    results = finder.find_all_funds_by_tickers(etf_data.head(30), max_funds=30)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    finder.save_results(results)
    
    if results:
        print(f"\n‚úÖ –ù–∞–π–¥–µ–Ω–Ω—ã–µ –º–∞–ø–ø–∏–Ω–≥–∏:")
        for ticker, data in results.items():
            print(f"   {ticker} -> ID {data['fund_id']} (–°–ß–ê: {data['nav']/1e9:.1f} –º–ª—Ä–¥ ‚ÇΩ)")
        
        # –û–±–Ω–æ–≤–ª—è–µ–º investfunds_parser.py
        finder.update_investfunds_parser_mapping(results)
        
        print(f"\nüéØ –ò—Ç–æ–≥–∏:")
        print(f"   üìä –ù–∞–π–¥–µ–Ω–æ: {len(results)} –∏–∑ 30 —Ñ–æ–Ω–¥–æ–≤")
        print(f"   üìà –£—Å–ø–µ—à–Ω–æ—Å—Ç—å: {len(results)/30*100:.1f}%")
    else:
        print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ –Ω–∏ –æ–¥–Ω–æ–≥–æ —Ñ–æ–Ω–¥–∞")
    
    print(f"\nüíæ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ ticker_search_results.json")

if __name__ == "__main__":
    main()