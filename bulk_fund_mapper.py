#!/usr/bin/env python3
"""
–ë—ã—Å—Ç—Ä—ã–π –º–∞—Å—Å–æ–≤—ã–π –º–∞–ø–ø–µ—Ä –ë–ü–ò–§ –Ω–∞ investfunds.ru
–ó–∞–≥—Ä—É–∂–∞–µ—Ç –≤—Å–µ —Ñ–æ–Ω–¥—ã –æ–¥–∏–Ω —Ä–∞–∑, –∑–∞—Ç–µ–º –∏—â–µ—Ç –ª–æ–∫–∞–ª—å–Ω–æ
"""

import pandas as pd
import requests
from bs4 import BeautifulSoup
import re
import time
import json
from typing import Dict, List, Optional, Tuple
import logging
from pathlib import Path
import difflib
from dataclasses import dataclass

@dataclass
class FundInfo:
    """–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Ñ–æ–Ω–¥–µ —Å investfunds.ru"""
    fund_id: int
    name: str
    url: str
    type: str = ""

class BulkFundMapper:
    """–ë—ã—Å—Ç—Ä—ã–π –º–∞—Å—Å–æ–≤—ã–π –º–∞–ø–ø–µ—Ä –ë–ü–ò–§"""
    
    def __init__(self):
        self.base_url = "https://investfunds.ru"
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
        })
        
        self.logger = self._setup_logger()
        self.all_funds = []  # –ë–∞–∑–∞ –≤—Å–µ—Ö —Ñ–æ–Ω–¥–æ–≤ —Å —Å–∞–π—Ç–∞
        
    def _setup_logger(self) -> logging.Logger:
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–≥–µ—Ä–∞"""
        logger = logging.getLogger('BulkFundMapper')
        logger.setLevel(logging.INFO)
        
        if not logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
            handler.setFormatter(formatter)
            logger.addHandler(handler)
            
        return logger
    
    def load_all_funds(self) -> List[FundInfo]:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –≤—Å–µ—Ö —Ñ–æ–Ω–¥–∞—Ö —Å —Å–∞–π—Ç–∞"""
        
        self.logger.info("üì• –ó–∞–≥—Ä—É–∂–∞–µ–º —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö —Ñ–æ–Ω–¥–æ–≤ —Å investfunds.ru...")
        
        funds = []
        
        try:
            # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É —Å–æ —Å–ø–∏—Å–∫–æ–º —Ñ–æ–Ω–¥–æ–≤
            funds_url = f"{self.base_url}/funds/"
            response = self.session.get(funds_url, timeout=30)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # –ò—â–µ–º –≤—Å–µ —Å—Å—ã–ª–∫–∏ –Ω–∞ —Ñ–æ–Ω–¥—ã
            fund_links = soup.find_all('a', href=re.compile(r'/funds/\d+/'))
            
            self.logger.info(f"üîç –ù–∞–π–¥–µ–Ω–æ {len(fund_links)} —Å—Å—ã–ª–æ–∫ –Ω–∞ —Ñ–æ–Ω–¥—ã")
            
            seen_ids = set()
            
            for link in fund_links:
                try:
                    href = link.get('href', '')
                    fund_id_match = re.search(r'/funds/(\d+)/', href)
                    
                    if not fund_id_match:
                        continue
                    
                    fund_id = int(fund_id_match.group(1))
                    
                    # –ò–∑–±–µ–≥–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
                    if fund_id in seen_ids:
                        continue
                    seen_ids.add(fund_id)
                    
                    fund_name = link.get_text(strip=True)
                    
                    if fund_name:
                        funds.append(FundInfo(
                            fund_id=fund_id,
                            name=fund_name,
                            url=f"{self.base_url}{href}"
                        ))
                
                except Exception as e:
                    self.logger.debug(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å—Å—ã–ª–∫–∏: {e}")
                    continue
            
            self.logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(funds)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ñ–æ–Ω–¥–æ–≤")
            self.all_funds = funds
            
            return funds
            
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Å–ø–∏—Å–∫–∞ —Ñ–æ–Ω–¥–æ–≤: {e}")
            return []
    
    def find_best_matches(self, ticker: str, fund_name: str, max_results: int = 5) -> List[Tuple[FundInfo, float]]:
        """–ù–∞—Ö–æ–¥–∏—Ç –ª—É—á—à–∏–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è —Å—Ä–µ–¥–∏ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö —Ñ–æ–Ω–¥–æ–≤"""
        
        matches = []
        
        # –û—á–∏—â–∞–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ –¥–ª—è –ø–æ–∏—Å–∫–∞
        clean_search_name = self._clean_fund_name(fund_name)
        
        for fund in self.all_funds:
            # –ù–µ—Å–∫–æ–ª—å–∫–æ —Å—Ç—Ä–∞—Ç–µ–≥–∏–π –ø–æ–∏—Å–∫–∞
            
            # 1. –ü—Ä—è–º–æ–µ –≤—Ö–æ–∂–¥–µ–Ω–∏–µ —Ç–∏–∫–µ—Ä–∞ –≤ –Ω–∞–∑–≤–∞–Ω–∏–µ
            ticker_score = 0
            if ticker.upper() in fund.name.upper():
                ticker_score = 0.9
                
            # 2. –°—Ö–æ–∂–µ—Å—Ç—å –æ—á–∏—â–µ–Ω–Ω—ã—Ö –Ω–∞–∑–≤–∞–Ω–∏–π
            clean_fund_name = self._clean_fund_name(fund.name)
            name_similarity = difflib.SequenceMatcher(None, clean_search_name.lower(), clean_fund_name.lower()).ratio()
            
            # 3. –°–æ–≤–ø–∞–¥–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
            search_keywords = set(re.findall(r'\b\w{3,}\b', clean_search_name.lower()))
            fund_keywords = set(re.findall(r'\b\w{3,}\b', clean_fund_name.lower()))
            
            keyword_score = 0
            if search_keywords and fund_keywords:
                keyword_score = len(search_keywords & fund_keywords) / len(search_keywords | fund_keywords)
            
            # –ò—Ç–æ–≥–æ–≤—ã–π —Å–∫–æ—Ä
            final_score = max(ticker_score, name_similarity, keyword_score * 0.8)
            
            if final_score > 0.3:  # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥
                matches.append((fund, final_score))
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Å–∫–æ—Ä—É
        matches.sort(key=lambda x: x[1], reverse=True)
        
        return matches[:max_results]
    
    def _clean_fund_name(self, name: str) -> str:
        """–û—á–∏—â–∞–µ—Ç –Ω–∞–∑–≤–∞–Ω–∏–µ —Ñ–æ–Ω–¥–∞ –¥–ª—è –ø–æ–∏—Å–∫–∞"""
        
        clean = name
        
        # –£–±–∏—Ä–∞–µ–º –æ–±—â–∏–µ —Å–ª–æ–≤–∞ –∏ –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä—ã
        patterns_to_remove = [
            r'\b(–ë–ü–ò–§|ETF|–§–æ–Ω–¥|–£–ö|–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ)\b',
            r'[¬´¬ª"\'()].*?[¬´¬ª"\'()]',
            r'\([^)]*\)',
            r'‚Äì\s*\d+',  # –£–±–∏—Ä–∞–µ–º –Ω–æ–º–µ—Ä–∞ –ø–æ—Å–ª–µ —Ç–∏—Ä–µ
        ]
        
        for pattern in patterns_to_remove:
            clean = re.sub(pattern, '', clean, flags=re.IGNORECASE)
        
        # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã
        clean = re.sub(r'\s+', ' ', clean).strip()
        
        return clean
    
    def auto_map_funds(self, etf_data: pd.DataFrame, confidence_threshold: float = 0.8) -> Dict[str, int]:
        """–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –º–∞–ø–ø–∏—Ç —Ñ–æ–Ω–¥—ã —Å –≤—ã—Å–æ–∫–æ–π —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å—é"""
        
        if not self.all_funds:
            self.load_all_funds()
        
        mapping = {}
        stats = {
            'high_confidence': 0,
            'medium_confidence': 0,
            'low_confidence': 0,
            'no_matches': 0
        }
        
        self.logger.info(f"ü§ñ –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –º–∞–ø–ø–∏–Ω–≥ {len(etf_data)} —Ñ–æ–Ω–¥–æ–≤...")
        self.logger.info(f"üìä –ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö: {len(self.all_funds)} —Ñ–æ–Ω–¥–æ–≤ —Å investfunds.ru")
        
        results = []
        
        for idx, row in etf_data.iterrows():
            ticker = row['ticker']
            fund_name = row.get('full_name', row.get('short_name', ticker))
            
            matches = self.find_best_matches(ticker, fund_name)
            
            if matches:
                best_match, confidence = matches[0]
                
                result = {
                    'ticker': ticker,
                    'search_name': fund_name,
                    'best_match_id': best_match.fund_id,
                    'best_match_name': best_match.name,
                    'confidence': confidence,
                    'alternatives': [(m[0].fund_id, m[0].name, m[1]) for m in matches[1:3]]
                }
                
                if confidence >= confidence_threshold:
                    mapping[ticker] = best_match.fund_id
                    stats['high_confidence'] += 1
                    result['status'] = 'auto_mapped'
                    self.logger.info(f"‚úÖ {ticker}: {best_match.fund_id} (—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {confidence:.2f})")
                elif confidence >= 0.6:
                    stats['medium_confidence'] += 1
                    result['status'] = 'needs_review'
                    self.logger.info(f"‚ö†Ô∏è  {ticker}: {best_match.fund_id} (—Ç—Ä–µ–±—É–µ—Ç –ø—Ä–æ–≤–µ—Ä–∫–∏: {confidence:.2f})")
                else:
                    stats['low_confidence'] += 1
                    result['status'] = 'low_confidence'
                    self.logger.info(f"‚ùì {ticker}: {best_match.fund_id} (–Ω–∏–∑–∫–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {confidence:.2f})")
            else:
                stats['no_matches'] += 1
                result = {
                    'ticker': ticker,
                    'search_name': fund_name,
                    'status': 'no_matches'
                }
                self.logger.warning(f"‚ùå {ticker}: —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")
            
            results.append(result)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–µ—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        results_df = pd.DataFrame(results)
        results_df.to_csv('fund_mapping_results.csv', index=False, encoding='utf-8')
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏—Ç–æ–≥–æ–≤—ã–π –º–∞–ø–ø–∏–Ω–≥
        mapping_data = {
            'mapping': mapping,
            'stats': stats,
            'confidence_threshold': confidence_threshold,
            'total_funds_analyzed': len(etf_data),
            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S')
        }
        
        with open('auto_fund_mapping.json', 'w', encoding='utf-8') as f:
            json.dump(mapping_data, f, indent=2, ensure_ascii=False)
        
        # –í—ã–≤–æ–¥–∏–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        self.logger.info(f"\nüìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –º–∞–ø–ø–∏–Ω–≥–∞:")
        self.logger.info(f"   ‚úÖ –í—ã—Å–æ–∫–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {stats['high_confidence']}")
        self.logger.info(f"   ‚ö†Ô∏è  –¢—Ä–µ–±—É–µ—Ç –ø—Ä–æ–≤–µ—Ä–∫–∏: {stats['medium_confidence']}")
        self.logger.info(f"   ‚ùì –ù–∏–∑–∫–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {stats['low_confidence']}")
        self.logger.info(f"   ‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–æ: {stats['no_matches']}")
        self.logger.info(f"   üìà –ü—Ä–æ—Ü–µ–Ω—Ç –∞–≤—Ç–æ–º–∞–ø–ø–∏–Ω–≥–∞: {stats['high_confidence']/len(etf_data)*100:.1f}%")
        
        return mapping
    
    def generate_review_report(self) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç—á–µ—Ç –¥–ª—è —Ä—É—á–Ω–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏"""
        
        try:
            results_df = pd.read_csv('fund_mapping_results.csv')
        except:
            self.logger.error("–§–∞–π–ª —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω")
            return ""
        
        report = []
        report.append("# üìã –û—Ç—á–µ—Ç –ø–æ –º–∞–ø–ø–∏–Ω–≥—É –ë–ü–ò–§ –Ω–∞ investfunds.ru\n")
        
        # –§–æ–Ω–¥—ã, —Ç—Ä–µ–±—É—é—â–∏–µ –ø—Ä–æ–≤–µ—Ä–∫–∏
        needs_review = results_df[results_df['status'] == 'needs_review']
        if not needs_review.empty:
            report.append("## ‚ö†Ô∏è –¢—Ä–µ–±—É—é—Ç —Ä—É—á–Ω–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏:\n")
            for _, row in needs_review.iterrows():
                report.append(f"### {row['ticker']} - {row['search_name']}")
                report.append(f"**–ü—Ä–µ–¥–ª–∞–≥–∞–µ–º—ã–π ID:** {row['best_match_id']}")
                report.append(f"**–ù–∞–∑–≤–∞–Ω–∏–µ –Ω–∞ —Å–∞–π—Ç–µ:** {row['best_match_name']}")
                report.append(f"**–£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å:** {row['confidence']:.2f}")
                report.append(f"**URL:** https://investfunds.ru/funds/{row['best_match_id']}/")
                report.append("")
        
        # –§–æ–Ω–¥—ã —Å –Ω–∏–∑–∫–æ–π —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å—é
        low_confidence = results_df[results_df['status'] == 'low_confidence']
        if not low_confidence.empty:
            report.append("## ‚ùì –ù–∏–∑–∫–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å:\n")
            for _, row in low_confidence.iterrows():
                report.append(f"- **{row['ticker']}**: {row['best_match_name']} (ID: {row['best_match_id']}, —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {row['confidence']:.2f})")
        
        # –ù–µ –Ω–∞–π–¥–µ–Ω–Ω—ã–µ —Ñ–æ–Ω–¥—ã
        no_matches = results_df[results_df['status'] == 'no_matches']
        if not no_matches.empty:
            report.append("## ‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω—ã —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è:\n")
            for _, row in no_matches.iterrows():
                report.append(f"- **{row['ticker']}**: {row['search_name']}")
        
        report_text = "\n".join(report)
        
        with open('fund_mapping_review.md', 'w', encoding='utf-8') as f:
            f.write(report_text)
        
        return report_text

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ ETF
    data_files = list(Path('.').glob('enhanced_etf_data_*.csv'))
    if not data_files:
        data_files = list(Path('.').glob('full_moex_etf_data_*.csv'))
    
    if not data_files:
        print("‚ùå –§–∞–π–ª—ã —Å –¥–∞–Ω–Ω—ã–º–∏ ETF –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
        return
    
    data_file = max(data_files, key=lambda x: x.stat().st_mtime)
    print(f"üìä –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ {data_file}")
    
    etf_data = pd.read_csv(data_file)
    print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(etf_data)} ETF")
    
    # –°–æ–∑–¥–∞–µ–º –º–∞–ø–ø–µ—Ä
    mapper = BulkFundMapper()
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤—Å–µ —Ñ–æ–Ω–¥—ã
    mapper.load_all_funds()
    
    # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –º–∞–ø–ø–∏–Ω–≥
    mapping = mapper.auto_map_funds(etf_data, confidence_threshold=0.8)
    
    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç—á–µ—Ç –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
    mapper.generate_review_report()
    
    print(f"\nüéâ –ú–∞–ø–ø–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω!")
    print(f"üìÑ –§–∞–π–ª—ã —Å–æ–∑–¥–∞–Ω—ã:")
    print(f"   - auto_fund_mapping.json (–∏—Ç–æ–≥–æ–≤—ã–π –º–∞–ø–ø–∏–Ω–≥)")
    print(f"   - fund_mapping_results.csv (–¥–µ—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã)")
    print(f"   - fund_mapping_review.md (–æ—Ç—á–µ—Ç –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏)")

if __name__ == "__main__":
    main()