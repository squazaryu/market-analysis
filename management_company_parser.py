#!/usr/bin/env python3
"""
–ü–∞—Ä—Å–µ—Ä –¥–∞–Ω–Ω—ã—Ö –°–ß–ê —Å —Å–∞–π—Ç–æ–≤ —É–ø—Ä–∞–≤–ª—è—é—â–∏—Ö –∫–æ–º–ø–∞–Ω–∏–π
–ë–æ–ª–µ–µ –Ω–∞–¥–µ–∂–Ω—ã–π –∏—Å—Ç–æ—á–Ω–∏–∫, —á–µ–º investfunds.ru
"""

import pandas as pd
import requests
from bs4 import BeautifulSoup
import re
import time
import json
from typing import Dict, List, Optional
import logging
from pathlib import Path

class ManagementCompanyParser:
    """–ü–∞—Ä—Å–µ—Ä –¥–∞–Ω–Ω—ã—Ö —Å —Å–∞–π—Ç–æ–≤ –£–ö"""
    
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
        })
        
        self.logger = self._setup_logger()
        
        # –ú–∞–ø–ø–∏–Ω–≥ —É–ø—Ä–∞–≤–ª—è—é—â–∏—Ö –∫–æ–º–ø–∞–Ω–∏–π –Ω–∞ –∏—Ö —Å–∞–π—Ç—ã
        self.management_companies = {
            '–ê–ª—å—Ñ–∞-–ö–∞–ø–∏—Ç–∞–ª': {
                'base_url': 'https://alfabank.ru',
                'funds_url': 'https://alfabank.ru/make-money/pif/',
                'parser': self._parse_alfa_capital
            },
            '–°–±–µ—Ä–±–∞–Ω–∫ –ê–ú': {
                'base_url': 'https://sberassetmanagement.ru',
                'funds_url': 'https://sberassetmanagement.ru/funds/',
                'parser': self._parse_sberbank
            },
            '–¢-–ö–∞–ø–∏—Ç–∞–ª': {
                'base_url': 'https://t-capital.pro',
                'funds_url': 'https://t-capital.pro/funds/',
                'parser': self._parse_t_capital
            },
            '–í–ò–ú': {
                'base_url': 'https://vim.capital',
                'funds_url': 'https://vim.capital/funds/',
                'parser': self._parse_vim
            }
        }
        
    def _setup_logger(self) -> logging.Logger:
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–≥–µ—Ä–∞"""
        logger = logging.getLogger('ManagementCompanyParser')
        logger.setLevel(logging.INFO)
        
        if not logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
            handler.setFormatter(formatter)
            logger.addHandler(handler)
            
        return logger
    
    def _parse_alfa_capital(self, ticker: str, isin: str) -> Optional[Dict]:
        """–ü–∞—Ä—Å–∏—Ç –¥–∞–Ω–Ω—ã–µ —Å —Å–∞–π—Ç–∞ –ê–ª—å—Ñ–∞-–ö–∞–ø–∏—Ç–∞–ª"""
        
        try:
            # –ê–ª—å—Ñ–∞-–ö–∞–ø–∏—Ç–∞–ª –ø—É–±–ª–∏–∫—É–µ—Ç –¥–∞–Ω–Ω—ã–µ –≤ —Ñ–æ—Ä–º–∞—Ç–µ –æ—Ç—á–µ—Ç–æ–≤
            funds_url = "https://alfabank.ru/make-money/pif/"
            
            response = self.session.get(funds_url, timeout=15)
            if response.status_code != 200:
                return None
                
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # –ò—â–µ–º —Ç–∞–±–ª–∏—Ü—ã —Å –¥–∞–Ω–Ω—ã–º–∏ —Ñ–æ–Ω–¥–æ–≤
            tables = soup.find_all('table')
            
            for table in tables:
                rows = table.find_all('tr')
                for row in rows:
                    cells = row.find_all(['td', 'th'])
                    row_text = ' '.join([cell.get_text(strip=True) for cell in cells])
                    
                    # –ò—â–µ–º –ø–æ —Ç–∏–∫–µ—Ä—É –∏–ª–∏ ISIN
                    if ticker.upper() in row_text.upper() or isin in row_text:
                        # –ü—ã—Ç–∞–µ–º—Å—è –∏–∑–≤–ª–µ—á—å –°–ß–ê –∏ —Ü–µ–Ω—É –ø–∞—è
                        nav_match = re.search(r'(\d{1,3}(?:[\s,]\d{3})*(?:\.\d+)?)', row_text)
                        if nav_match:
                            nav_value = float(nav_match.group(1).replace(' ', '').replace(',', ''))
                            
                            return {
                                'ticker': ticker,
                                'nav': nav_value * 1000000,  # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ —Ä—É–±–ª–∏
                                'unit_price': 0,  # –ù—É–∂–Ω–æ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ –∏—Å–∫–∞—Ç—å
                                'source': '–ê–ª—å—Ñ–∞-–ö–∞–ø–∏—Ç–∞–ª',
                                'url': funds_url
                            }
                            
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –ê–ª—å—Ñ–∞-–ö–∞–ø–∏—Ç–∞–ª –¥–ª—è {ticker}: {e}")
        
        return None
    
    def _parse_sberbank(self, ticker: str, isin: str) -> Optional[Dict]:
        """–ü–∞—Ä—Å–∏—Ç –¥–∞–Ω–Ω—ã–µ —Å —Å–∞–π—Ç–∞ –°–±–µ—Ä–±–∞–Ω–∫ –ê–ú"""
        
        try:
            # –°–±–µ—Ä–±–∞–Ω–∫ –ê–ú –∏–º–µ–µ—Ç API –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö
            api_url = f"https://sberassetmanagement.ru/api/funds/"
            
            response = self.session.get(api_url, timeout=15)
            if response.status_code == 200:
                try:
                    data = response.json()
                    
                    # –ò—â–µ–º —Ñ–æ–Ω–¥ –ø–æ —Ç–∏–∫–µ—Ä—É –∏–ª–∏ ISIN
                    for fund in data.get('funds', []):
                        if (fund.get('ticker', '').upper() == ticker.upper() or 
                            fund.get('isin') == isin):
                            
                            return {
                                'ticker': ticker,
                                'nav': fund.get('nav', 0),
                                'unit_price': fund.get('unit_price', 0),
                                'source': '–°–±–µ—Ä–±–∞–Ω–∫ –ê–ú',
                                'url': f"https://sberassetmanagement.ru/funds/{fund.get('id', '')}"
                            }
                            
                except json.JSONDecodeError:
                    pass
            
            # Fallback –Ω–∞ –ø–∞—Ä—Å–∏–Ω–≥ HTML
            html_url = "https://sberassetmanagement.ru/funds/"
            response = self.session.get(html_url, timeout=15)
            
            if response.status_code == 200:
                soup = BeautifulSoup(response.content, 'html.parser')
                
                # –ò—â–µ–º –∫–∞—Ä—Ç–æ—á–∫–∏ —Ñ–æ–Ω–¥–æ–≤
                fund_cards = soup.find_all(['div', 'section'], class_=re.compile(r'fund|card'))
                
                for card in fund_cards:
                    card_text = card.get_text()
                    if ticker.upper() in card_text.upper():
                        # –ò—â–µ–º —á–∏—Å–ª–æ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
                        numbers = re.findall(r'(\d{1,3}(?:[\s,]\d{3})*(?:\.\d+)?)', card_text)
                        if numbers:
                            # –ü–µ—Ä–≤–æ–µ –±–æ–ª—å—à–æ–µ —á–∏—Å–ª–æ - –≤–µ—Ä–æ—è—Ç–Ω–æ –°–ß–ê
                            for num_str in numbers:
                                try:
                                    num_val = float(num_str.replace(' ', '').replace(',', ''))
                                    if num_val > 1000000:  # –ë–æ–ª—å—à–µ –º–∏–ª–ª–∏–æ–Ω–∞ - –ø–æ—Ö–æ–∂–µ –Ω–∞ –°–ß–ê
                                        return {
                                            'ticker': ticker,
                                            'nav': num_val,
                                            'unit_price': 0,
                                            'source': '–°–±–µ—Ä–±–∞–Ω–∫ –ê–ú',
                                            'url': html_url
                                        }
                                except ValueError:
                                    continue
                            
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –°–±–µ—Ä–±–∞–Ω–∫ –ê–ú –¥–ª—è {ticker}: {e}")
        
        return None
    
    def _parse_t_capital(self, ticker: str, isin: str) -> Optional[Dict]:
        """–ü–∞—Ä—Å–∏—Ç –¥–∞–Ω–Ω—ã–µ —Å —Å–∞–π—Ç–∞ –¢-–ö–∞–ø–∏—Ç–∞–ª"""
        
        try:
            funds_url = "https://t-capital.pro/funds/"
            
            response = self.session.get(funds_url, timeout=15)
            if response.status_code != 200:
                return None
                
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # –ò—â–µ–º —Å—Å—ã–ª–∫–∏ –Ω–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —Ñ–æ–Ω–¥—ã
            fund_links = soup.find_all('a', href=re.compile(r'/funds/'))
            
            for link in fund_links:
                link_text = link.get_text()
                if ticker.upper() in link_text.upper():
                    
                    # –ü–µ—Ä–µ—Ö–æ–¥–∏–º –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—É –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ —Ñ–æ–Ω–¥–∞
                    fund_url = link.get('href')
                    if not fund_url.startswith('http'):
                        fund_url = "https://t-capital.pro" + fund_url
                    
                    fund_response = self.session.get(fund_url, timeout=15)
                    if fund_response.status_code == 200:
                        fund_soup = BeautifulSoup(fund_response.content, 'html.parser')
                        
                        # –ò—â–µ–º –¥–∞–Ω–Ω—ã–µ –æ –°–ß–ê –∏ —Ü–µ–Ω–µ –ø–∞—è
                        fund_text = fund_soup.get_text()
                        
                        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –ø–æ–∏—Å–∫–∞ –°–ß–ê
                        nav_patterns = [
                            r'–°–ß–ê[:\s]*([0-9,.\s]+)',
                            r'—á–∏—Å—Ç—ã—Ö –∞–∫—Ç–∏–≤–æ–≤[:\s]*([0-9,.\s]+)',
                            r'–∞–∫—Ç–∏–≤—ã[:\s]*([0-9,.\s]+)'
                        ]
                        
                        for pattern in nav_patterns:
                            match = re.search(pattern, fund_text, re.IGNORECASE)
                            if match:
                                nav_str = match.group(1).strip()
                                nav_value = self._parse_number(nav_str)
                                
                                if nav_value and nav_value > 1000000:
                                    return {
                                        'ticker': ticker,
                                        'nav': nav_value,
                                        'unit_price': 0,  # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ –º–æ–∂–Ω–æ –∏—Å–∫–∞—Ç—å
                                        'source': '–¢-–ö–∞–ø–∏—Ç–∞–ª',
                                        'url': fund_url
                                    }
                    
                    time.sleep(1)
                    
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –¢-–ö–∞–ø–∏—Ç–∞–ª –¥–ª—è {ticker}: {e}")
        
        return None
    
    def _parse_vim(self, ticker: str, isin: str) -> Optional[Dict]:
        """–ü–∞—Ä—Å–∏—Ç –¥–∞–Ω–Ω—ã–µ —Å —Å–∞–π—Ç–∞ –í–ò–ú (–¥–ª—è LQDT)"""
        
        try:
            # –í–ò–ú –ø—É–±–ª–∏–∫—É–µ—Ç –æ—Ç—á–µ—Ç—ã –ø–æ —Ñ–æ–Ω–¥–∞–º
            funds_url = "https://vim.capital/funds/"
            
            response = self.session.get(funds_url, timeout=15)
            if response.status_code != 200:
                return None
                
            soup = BeautifulSoup(response.content, 'html.parser')
            page_text = soup.get_text()
            
            # –î–ª—è LQDT –∏—â–µ–º —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
            if ticker.upper() == 'LQDT' and 'LQDT' in page_text.upper():
                
                # –ò—â–µ–º –∞–∫—Ç—É–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –æ —Ñ–æ–Ω–¥–µ
                nav_patterns = [
                    r'LQDT.*?(\d{1,3}(?:[\s,]\d{3})*(?:\.\d+)?)',
                    r'–õ–∏–∫–≤–∏–¥–Ω–æ—Å—Ç—å.*?(\d{1,3}(?:[\s,]\d{3})*(?:\.\d+)?)'
                ]
                
                for pattern in nav_patterns:
                    matches = re.findall(pattern, page_text, re.IGNORECASE | re.DOTALL)
                    for match in matches:
                        nav_value = self._parse_number(match)
                        if nav_value and nav_value > 100000000:  # –ë–æ–ª—å—à–µ 100 –º–ª–Ω
                            return {
                                'ticker': ticker,
                                'nav': nav_value,
                                'unit_price': 0,
                                'source': '–í–ò–ú',
                                'url': funds_url
                            }
                            
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –í–ò–ú –¥–ª—è {ticker}: {e}")
        
        return None
    
    def _parse_number(self, text: str) -> Optional[float]:
        """–ü–∞—Ä—Å–∏—Ç —á–∏—Å–ª–æ –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
        
        if not text:
            return None
        
        try:
            # –£–±–∏—Ä–∞–µ–º –≤—Å–µ –∫—Ä–æ–º–µ —Ü–∏—Ñ—Ä, —Ç–æ—á–µ–∫ –∏ –∑–∞–ø—è—Ç—ã—Ö
            cleaned = re.sub(r'[^\d.,]', '', str(text).strip())
            
            if not cleaned:
                return None
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–∏
            if ',' in cleaned and '.' in cleaned:
                if cleaned.rfind(',') > cleaned.rfind('.'):
                    cleaned = cleaned.replace('.', '').replace(',', '.')
                else:
                    cleaned = cleaned.replace(',', '')
            elif ',' in cleaned:
                if cleaned.count(',') == 1 and len(cleaned.split(',')[1]) <= 2:
                    cleaned = cleaned.replace(',', '.')
                else:
                    cleaned = cleaned.replace(',', '')
            
            return float(cleaned)
            
        except (ValueError, AttributeError):
            return None
    
    def get_fund_data_by_management_company(self, ticker: str, isin: str, management_company: str) -> Optional[Dict]:
        """–ü–æ–ª—É—á–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ —Ñ–æ–Ω–¥–∞ —Å —Å–∞–π—Ç–∞ –£–ö"""
        
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ –£–ö
        uc_mappings = {
            '–ê–ª—å—Ñ–∞-–ö–∞–ø–∏—Ç–∞–ª': ['–ê–ª—å—Ñ–∞-–ö–∞–ø–∏—Ç–∞–ª', 'A-Capital', '–ê–õ–¨–§–ê'],
            '–°–±–µ—Ä–±–∞–Ω–∫ –ê–ú': ['–°–±–µ—Ä–±–∞–Ω–∫ –ê–ú', 'Sberbank AM', '–°–ë–ï–†'],
            '–¢-–ö–∞–ø–∏—Ç–∞–ª': ['–¢-–ö–∞–ø–∏—Ç–∞–ª', 'T-Capital', '–¢–ò–ù–¨–ö–û–§–§'],
            '–í–ò–ú': ['–í–ò–ú', 'VIM', '–í–ò–ú –ò–Ω–≤–µ—Å—Ç–∏—Ü–∏–∏']
        }
        
        matched_uc = None
        for uc_name, aliases in uc_mappings.items():
            if any(alias.upper() in management_company.upper() for alias in aliases):
                matched_uc = uc_name
                break
        
        if not matched_uc or matched_uc not in self.management_companies:
            return None
        
        parser_func = self.management_companies[matched_uc]['parser']
        
        try:
            return parser_func(ticker, isin)
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ {matched_uc} –¥–ª—è {ticker}: {e}")
            return None
    
    def batch_parse_funds(self, etf_data: pd.DataFrame) -> Dict[str, Dict]:
        """–ú–∞—Å—Å–æ–≤—ã–π –ø–∞—Ä—Å–∏–Ω–≥ —Ñ–æ–Ω–¥–æ–≤ —Å —Å–∞–π—Ç–æ–≤ –£–ö"""
        
        results = {}
        
        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º —Ñ–æ–Ω–¥—ã –ø–æ –£–ö –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
        uc_groups = etf_data.groupby('management_company')
        
        for uc_name, group in uc_groups:
            self.logger.info(f"üè¢ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –£–ö: {uc_name} ({len(group)} —Ñ–æ–Ω–¥–æ–≤)")
            
            for _, row in group.iterrows():
                ticker = row['ticker']
                isin = row.get('isin', '')
                
                self.logger.info(f"  üîç –ò—â–µ–º {ticker}")
                
                fund_data = self.get_fund_data_by_management_company(ticker, isin, uc_name)
                
                if fund_data:
                    results[ticker] = fund_data
                    self.logger.info(f"  ‚úÖ {ticker}: –°–ß–ê {fund_data['nav']/1e9:.1f} –º–ª—Ä–¥ ‚ÇΩ")
                else:
                    self.logger.info(f"  ‚ùå {ticker}: –Ω–µ –Ω–∞–π–¥–µ–Ω")
                
                time.sleep(1)  # –ü–∞—É–∑–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
        
        return results

def main():
    """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ"""
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ ETF
    data_files = list(Path('.').glob('enhanced_etf_data_*.csv'))
    if not data_files:
        data_files = list(Path('.').glob('full_moex_etf_data_*.csv'))
    
    if not data_files:
        print("‚ùå –§–∞–π–ª—ã —Å –¥–∞–Ω–Ω—ã–º–∏ ETF –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
        return
    
    latest_data = max(data_files, key=lambda x: x.stat().st_mtime)
    etf_data = pd.read_csv(latest_data)
    
    print(f"üìä –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(etf_data)} —Ñ–æ–Ω–¥–æ–≤")
    
    # –°–æ–∑–¥–∞–µ–º –ø–∞—Ä—Å–µ—Ä
    parser = ManagementCompanyParser()
    
    # –¢–µ—Å—Ç–∏—Ä—É–µ–º –Ω–∞ –Ω–µ–±–æ–ª—å—à–æ–π –≤—ã–±–æ—Ä–∫–µ
    test_funds = etf_data[etf_data['ticker'].isin(['LQDT', 'AKMM', 'SBMM', 'TPAY'])].copy()
    
    print(f"üß™ –¢–µ—Å—Ç–∏—Ä—É–µ–º –Ω–∞ {len(test_funds)} —Ñ–æ–Ω–¥–∞—Ö")
    
    results = parser.batch_parse_funds(test_funds)
    
    print(f"\nüìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã:")
    for ticker, data in results.items():
        print(f"  ‚úÖ {ticker}: {data['nav']/1e9:.1f} –º–ª—Ä–¥ ‚ÇΩ (–∏—Å—Ç–æ—á–Ω–∏–∫: {data['source']})")
    
    print(f"\nüéØ –ü–æ–∫—Ä—ã—Ç–∏–µ: {len(results)}/{len(test_funds)} = {len(results)/len(test_funds)*100:.1f}%")

if __name__ == "__main__":
    main()