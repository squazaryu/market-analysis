#!/usr/bin/env python3
"""
–°–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ —Ñ–æ–Ω–¥–æ–≤ –Ω–∞ investfunds.ru
–ë–æ–ª–µ–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –ø–æ–∏—Å–∫—É —Ä–µ–∞–ª—å–Ω—ã—Ö —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–π
"""

import requests
from bs4 import BeautifulSoup
import re
import pandas as pd
import time
import json
from typing import Dict, List, Optional, Set
from pathlib import Path
import concurrent.futures
from threading import Lock

class SystematicFundDiscovery:
    """–°–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ —Ñ–æ–Ω–¥–æ–≤"""
    
    def __init__(self, max_workers: int = 5):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
        })
        self.base_url = "https://investfunds.ru"
        self.max_workers = max_workers
        self.found_mappings = {}
        self.lock = Lock()
        
    def extract_ticker_from_page(self, soup: BeautifulSoup, page_text: str) -> Optional[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–∏–∫–µ—Ä —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Ñ–æ–Ω–¥–∞"""
        
        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –ø–æ–∏—Å–∫–∞ —Ç–∏–∫–µ—Ä–∞
        ticker_patterns = [
            r'\b([A-Z]{4,6})\b',  # 4-6 –∑–∞–≥–ª–∞–≤–Ω—ã—Ö –±—É–∫–≤
            r'—Ç–∏–∫–µ—Ä[:\s]*([A-Z]+)',
            r'—Å–∏–º–≤–æ–ª[:\s]*([A-Z]+)',
            r'–∫–æ–¥[:\s]*([A-Z]+)',
        ]
        
        potential_tickers = set()
        
        for pattern in ticker_patterns:
            matches = re.findall(pattern, page_text.upper())
            for match in matches:
                if 3 <= len(match) <= 6:  # –†–∞–∑—É–º–Ω–∞—è –¥–ª–∏–Ω–∞ —Ç–∏–∫–µ—Ä–∞
                    potential_tickers.add(match)
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –≤ –∑–∞–≥–æ–ª–æ–≤–∫–µ
        title = soup.find('h1')
        if title:
            title_text = title.get_text().upper()
            # –ò—â–µ–º –≤ —Å–∫–æ–±–∫–∞—Ö –∏–ª–∏ –ø–æ—Å–ª–µ –∑–∞–ø—è—Ç–æ–π
            bracket_match = re.search(r'[(),]\s*([A-Z]{3,6})', title_text)
            if bracket_match:
                potential_tickers.add(bracket_match.group(1))
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º –æ—á–µ–≤–∏–¥–Ω–æ –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ —Ç–∏–∫–µ—Ä—ã
        exclude_words = {'RUB', 'USD', 'EUR', 'HTML', 'HTTP', 'HTTPS', 'WWW', 'COM', 'ORG'}
        valid_tickers = [t for t in potential_tickers if t not in exclude_words]
        
        return valid_tickers[0] if valid_tickers else None
    
    def extract_nav_from_page(self, page_text: str) -> Optional[float]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –°–ß–ê —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
        
        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –ø–æ–∏—Å–∫–∞ –°–ß–ê
        nav_patterns = [
            r'–°–ß–ê[:\s]*([0-9,.\s]+)',
            r'—á–∏—Å—Ç—ã—Ö –∞–∫—Ç–∏–≤–æ–≤[:\s]*([0-9,.\s]+)',
            r'–∞–∫—Ç–∏–≤—ã[:\s]*([0-9,.\s]+)',
            r'(\d{1,3}(?:,\d{3})*(?:\.\d{2})?)',  # –ë–æ–ª—å—à–∏–µ —á–∏—Å–ª–∞ —Å –∑–∞–ø—è—Ç—ã–º–∏
        ]
        
        for pattern in nav_patterns:
            matches = re.findall(pattern, page_text, re.IGNORECASE)
            for match in matches:
                try:
                    # –û—á–∏—â–∞–µ–º —á–∏—Å–ª–æ
                    cleaned = re.sub(r'[^\d.]', '', match.replace(',', ''))
                    if cleaned:
                        value = float(cleaned)
                        # –°–ß–ê –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –±–æ–ª—å—à–µ 1 –º–ª–Ω —Ä—É–±–ª–µ–π
                        if value > 1_000_000:
                            return value
                except ValueError:
                    continue
        
        return None
    
    def check_fund_id(self, fund_id: int) -> Optional[Dict]:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π ID —Ñ–æ–Ω–¥–∞"""
        
        try:
            url = f"{self.base_url}/funds/{fund_id}/"
            response = self.session.get(url, timeout=10)
            
            if response.status_code != 200:
                return None
            
            soup = BeautifulSoup(response.content, 'html.parser')
            page_text = soup.get_text()
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞–Ω–Ω—ã–µ
            ticker = self.extract_ticker_from_page(soup, page_text)
            nav = self.extract_nav_from_page(page_text)
            
            # –ù–∞–∑–≤–∞–Ω–∏–µ —Ñ–æ–Ω–¥–∞
            title = soup.find('h1')
            fund_name = title.get_text(strip=True) if title else 'Unknown'
            
            if ticker:  # –ù–∞–π–¥–µ–Ω –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–π —Ç–∏–∫–µ—Ä
                fund_data = {
                    'fund_id': fund_id,
                    'ticker': ticker,
                    'name': fund_name,
                    'nav': nav or 0,
                    'url': url,
                    'page_length': len(page_text)
                }
                
                with self.lock:
                    print(f"‚úÖ ID {fund_id}: {ticker} - {fund_name[:50]}... (–°–ß–ê: {nav:,.0f})" if nav else f"‚ö†Ô∏è ID {fund_id}: {ticker} - {fund_name[:50]}... (–°–ß–ê: –Ω–µ –Ω–∞–π–¥–µ–Ω–∞)")
                
                return fund_data
            
        except Exception as e:
            if fund_id % 1000 == 0:  # –õ–æ–≥–∏—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –∫–∞–∂–¥—É—é 1000 –æ—à–∏–±–∫—É
                with self.lock:
                    print(f"‚ùå –û—à–∏–±–∫–∞ ID {fund_id}: {e}")
        
        return None
    
    def scan_id_range(self, start_id: int, end_id: int) -> List[Dict]:
        """–°–∫–∞–Ω–∏—Ä—É–µ—Ç –¥–∏–∞–ø–∞–∑–æ–Ω ID —Å –º–Ω–æ–≥–æ–ø–æ—Ç–æ—á–Ω–æ—Å—Ç—å—é"""
        
        print(f"üîç –°–∫–∞–Ω–∏—Ä—É–µ–º ID –æ—Ç {start_id} –¥–æ {end_id} ({end_id - start_id + 1} —Ñ–æ–Ω–¥–æ–≤)")
        
        found_funds = []
        
        with concurrent.futures.ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # –°–æ–∑–¥–∞–µ–º –∑–∞–¥–∞—á–∏ –¥–ª—è –≤—Å–µ—Ö ID
            futures = {executor.submit(self.check_fund_id, fund_id): fund_id 
                      for fund_id in range(start_id, end_id + 1)}
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            completed = 0
            for future in concurrent.futures.as_completed(futures):
                fund_data = future.result()
                if fund_data:
                    found_funds.append(fund_data)
                
                completed += 1
                if completed % 500 == 0:
                    print(f"üìä –ü—Ä–æ–≤–µ—Ä–µ–Ω–æ {completed}/{len(futures)} ID, –Ω–∞–π–¥–µ–Ω–æ {len(found_funds)} —Ñ–æ–Ω–¥–æ–≤")
        
        return found_funds
    
    def match_with_our_data(self, found_funds: List[Dict], etf_data: pd.DataFrame) -> Dict[str, int]:
        """–°–æ–ø–æ—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–∞–π–¥–µ–Ω–Ω—ã–µ —Ñ–æ–Ω–¥—ã —Å –Ω–∞—à–∏–º–∏ –¥–∞–Ω–Ω—ã–º–∏"""
        
        mappings = {}
        unmatched_our = set(etf_data['ticker'].tolist())
        unmatched_found = []
        
        print(f"\nüîÑ –°–æ–ø–æ—Å—Ç–∞–≤–ª—è–µ–º {len(found_funds)} –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —Ñ–æ–Ω–¥–æ–≤ —Å {len(etf_data)} –≤ –Ω–∞—à–µ–π –±–∞–∑–µ")
        
        for fund in found_funds:
            ticker = fund['ticker']
            fund_id = fund['fund_id']
            
            # –ü—Ä—è–º–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ —Ç–∏–∫–µ—Ä–∞
            if ticker in unmatched_our:
                mappings[ticker] = fund_id
                unmatched_our.remove(ticker)
                print(f"‚úÖ –ü—Ä—è–º–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ: {ticker} -> ID {fund_id}")
            else:
                unmatched_found.append(fund)
        
        print(f"\nüìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è:")
        print(f"  ‚úÖ –ü—Ä—è–º—ã—Ö —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π: {len(mappings)}")
        print(f"  ‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–æ –≤ –Ω–∞—à–µ–π –±–∞–∑–µ: {len(unmatched_found)}")
        print(f"  ‚ùì –ù–µ –Ω–∞–π–¥–µ–Ω–æ –Ω–∞ investfunds.ru: {len(unmatched_our)}")
        
        if unmatched_our:
            print(f"\n‚ùì –ù–∞—à–∏ —Ñ–æ–Ω–¥—ã –±–µ–∑ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π: {sorted(list(unmatched_our))[:10]}...")
        
        if unmatched_found:
            print(f"\n‚ùå –ù–∞–π–¥–µ–Ω–Ω—ã–µ —Ñ–æ–Ω–¥—ã –±–µ–∑ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π:")
            for fund in unmatched_found[:5]:
                print(f"  {fund['ticker']}: {fund['name'][:40]}...")
        
        return mappings
    
    def smart_scan(self, etf_data: pd.DataFrame, known_good_ids: List[int] = None) -> Dict[str, int]:
        """–£–º–Ω–æ–µ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ —Å —Ñ–æ–∫—É—Å–æ–º –Ω–∞ –ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤–Ω—ã–µ –¥–∏–∞–ø–∞–∑–æ–Ω—ã"""
        
        all_mappings = {}
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∏–∑–≤–µ—Å—Ç–Ω—ã–µ —Ö–æ—Ä–æ—à–∏–µ –º–∞–ø–ø–∏–Ω–≥–∏
        if known_good_ids:
            print(f"üéØ –ü—Ä–æ–≤–µ—Ä—è–µ–º {len(known_good_ids)} –∏–∑–≤–µ—Å—Ç–Ω—ã—Ö ID...")
            known_funds = []
            for fund_id in known_good_ids:
                fund_data = self.check_fund_id(fund_id)
                if fund_data:
                    known_funds.append(fund_data)
            
            known_mappings = self.match_with_our_data(known_funds, etf_data)
            all_mappings.update(known_mappings)
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤–Ω—ã–µ –¥–∏–∞–ø–∞–∑–æ–Ω—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏–∑–≤–µ—Å—Ç–Ω—ã—Ö ID
        if known_good_ids:
            min_id = min(known_good_ids)
            max_id = max(known_good_ids)
            
            # –°–∫–∞–Ω–∏—Ä—É–µ–º –¥–∏–∞–ø–∞–∑–æ–Ω –≤–æ–∫—Ä—É–≥ –∏–∑–≤–µ—Å—Ç–Ω—ã—Ö ID
            scan_ranges = [
                (max(1, min_id - 500), min_id - 1),           # –î–æ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–≥–æ
                (min_id + 1, max_id - 1),                     # –ú–µ–∂–¥—É –∏–∑–≤–µ—Å—Ç–Ω—ã–º–∏
                (max_id + 1, min(max_id + 2000, 15000))      # –ü–æ—Å–ª–µ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ
            ]
        else:
            # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –¥–∏–∞–ø–∞–∑–æ–Ω—ã
            scan_ranges = [
                (1, 2000),      # –°—Ç–∞—Ä—ã–µ —Ñ–æ–Ω–¥—ã
                (5000, 8000),   # –°—Ä–µ–¥–Ω–∏–µ
                (10000, 13000)  # –ù–æ–≤—ã–µ
            ]
        
        # –°–∫–∞–Ω–∏—Ä—É–µ–º –∫–∞–∂–¥—ã–π –¥–∏–∞–ø–∞–∑–æ–Ω
        for start_id, end_id in scan_ranges:
            print(f"\nüîç –°–∫–∞–Ω–∏—Ä—É–µ–º –¥–∏–∞–ø–∞–∑–æ–Ω {start_id}-{end_id}")
            
            found_funds = self.scan_id_range(start_id, end_id)
            new_mappings = self.match_with_our_data(found_funds, etf_data)
            all_mappings.update(new_mappings)
            
            print(f"üìà –î–æ–±–∞–≤–ª–µ–Ω–æ –Ω–æ–≤—ã—Ö –º–∞–ø–ø–∏–Ω–≥–æ–≤: {len(new_mappings)}")
            print(f"üìä –í—Å–µ–≥–æ –º–∞–ø–ø–∏–Ω–≥–æ–≤: {len(all_mappings)}")
            
            # –ü–∞—É–∑–∞ –º–µ–∂–¥—É –¥–∏–∞–ø–∞–∑–æ–Ω–∞–º–∏
            time.sleep(2)
        
        return all_mappings

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ ETF
    data_files = list(Path('.').glob('enhanced_etf_data_*.csv'))
    if not data_files:
        data_files = list(Path('.').glob('full_moex_etf_data_*.csv'))
    
    if not data_files:
        print("‚ùå –§–∞–π–ª—ã —Å –¥–∞–Ω–Ω—ã–º–∏ ETF –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
        return
    
    latest_data = max(data_files, key=lambda x: x.stat().st_mtime)
    etf_data = pd.read_csv(latest_data)
    
    print(f"üìä –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(etf_data)} —Ñ–æ–Ω–¥–æ–≤")
    
    # –°–æ–∑–¥–∞–µ–º —Å–∫–∞–Ω–µ—Ä
    scanner = SystematicFundDiscovery(max_workers=3)  # –ù–µ —Å–ª–∏—à–∫–æ–º –∞–≥—Ä–µ—Å—Å–∏–≤–Ω–æ
    
    # –ò–∑–≤–µ—Å—Ç–Ω—ã–µ —Ö–æ—Ä–æ—à–∏–µ ID
    known_ids = [5973, 6225, 10147, 11703, 11499]  # LQDT, AKMB, –∑–æ–ª–æ—Ç–æ, –∏ –¥—Ä.
    
    print(f"üöÄ –ó–∞–ø—É—Å–∫–∞–µ–º —Å–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ —Ñ–æ–Ω–¥–æ–≤")
    print(f"üéØ –¶–µ–ª–µ–≤–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ: {len(etf_data)} —Ñ–æ–Ω–¥–æ–≤")
    
    # –£–º–Ω–æ–µ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ
    final_mappings = scanner.smart_scan(etf_data, known_ids)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    results = {
        'timestamp': pd.Timestamp.now().isoformat(),
        'total_etf_funds': len(etf_data),
        'mapped_funds': len(final_mappings),
        'coverage_percent': len(final_mappings) / len(etf_data) * 100,
        'mappings': final_mappings
    }
    
    with open('systematic_discovery_results.json', 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    
    print(f"\nüéâ –ò–¢–û–ì–ò –°–ò–°–¢–ï–ú–ê–¢–ò–ß–ï–°–ö–û–ì–û –û–ë–ù–ê–†–£–ñ–ï–ù–ò–Ø:")
    print(f"  üìä –í—Å–µ–≥–æ –Ω–∞—à–∏—Ö —Ñ–æ–Ω–¥–æ–≤: {len(etf_data)}")
    print(f"  ‚úÖ –ù–∞–π–¥–µ–Ω–æ –º–∞–ø–ø–∏–Ω–≥–æ–≤: {len(final_mappings)}")
    print(f"  üìà –ü–æ–∫—Ä—ã—Ç–∏–µ: {len(final_mappings)/len(etf_data)*100:.1f}%")
    print(f"  üíæ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ systematic_discovery_results.json")
    
    if final_mappings:
        print(f"\nüèÜ –ù–∞–π–¥–µ–Ω–Ω—ã–µ –º–∞–ø–ø–∏–Ω–≥–∏:")
        for ticker, fund_id in sorted(final_mappings.items()):
            print(f"  '{ticker}': {fund_id},")

if __name__ == "__main__":
    main()