"""
ÐÐ½Ð°Ð»Ð¸Ð· Ð¿Ð¾Ð»Ð½Ð¾Ð³Ð¾ Ñ€ÐµÐµÑÑ‚Ñ€Ð° Ð‘ÐŸÐ˜Ð¤Ð¾Ð² Ð¸Ð· Ñ„Ð°Ð¹Ð»Ð° Ð¦Ð‘ Ð Ð¤
"""

import pandas as pd
import numpy as np
from datetime import datetime
from logger_config import logger
from etf_data_collector import ETFDataCollectorWithFallback
import time
import json


def load_cbr_pif_registry():
    """Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° Ð¿Ð¾Ð»Ð½Ð¾Ð³Ð¾ Ñ€ÐµÐµÑÑ‚Ñ€Ð° Ð‘ÐŸÐ˜Ð¤Ð¾Ð² Ð¸Ð· Ñ„Ð°Ð¹Ð»Ð° Ð¦Ð‘"""
    logger.info("Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° Ñ€ÐµÐµÑÑ‚Ñ€Ð° Ð‘ÐŸÐ˜Ð¤Ð¾Ð² Ð¸Ð· Ñ„Ð°Ð¹Ð»Ð° Ð¦Ð‘ Ð Ð¤...")
    
    try:
        # ÐŸÑ€Ð¾Ð±ÑƒÐµÐ¼ Ñ€Ð°Ð·Ð½Ñ‹Ðµ Ð²Ð°Ñ€Ð¸Ð°Ð½Ñ‚Ñ‹ Ñ‡Ñ‚ÐµÐ½Ð¸Ñ Excel Ñ„Ð°Ð¹Ð»Ð°
        file_path = "list_PIF.xlsx"
        
        # Ð¡Ð½Ð°Ñ‡Ð°Ð»Ð° Ð¿Ð¾ÑÐ¼Ð¾Ñ‚Ñ€Ð¸Ð¼ Ð½Ð° ÑÑ‚Ñ€ÑƒÐºÑ‚ÑƒÑ€Ñƒ Ñ„Ð°Ð¹Ð»Ð°
        xl_file = pd.ExcelFile(file_path)
        logger.info(f"Ð›Ð¸ÑÑ‚Ñ‹ Ð² Ñ„Ð°Ð¹Ð»Ðµ: {xl_file.sheet_names}")
        
        # Ð§Ð¸Ñ‚Ð°ÐµÐ¼ Ð¿ÐµÑ€Ð²Ñ‹Ð¹ Ð»Ð¸ÑÑ‚
        df = pd.read_excel(file_path, sheet_name=0)
        logger.info(f"Ð—Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½Ð¾ Ð·Ð°Ð¿Ð¸ÑÐµÐ¹: {len(df)}")
        logger.info(f"ÐšÐ¾Ð»Ð¾Ð½ÐºÐ¸: {list(df.columns)}")
        
        # ÐŸÐ¾ÐºÐ°Ð·Ñ‹Ð²Ð°ÐµÐ¼ Ð¿ÐµÑ€Ð²Ñ‹Ðµ Ð½ÐµÑÐºÐ¾Ð»ÑŒÐºÐ¾ Ð·Ð°Ð¿Ð¸ÑÐµÐ¹ Ð´Ð»Ñ Ð¿Ð¾Ð½Ð¸Ð¼Ð°Ð½Ð¸Ñ ÑÑ‚Ñ€ÑƒÐºÑ‚ÑƒÑ€Ñ‹
        logger.info("ÐŸÐµÑ€Ð²Ñ‹Ðµ 5 Ð·Ð°Ð¿Ð¸ÑÐµÐ¹:")
        for i, row in df.head().iterrows():
            logger.info(f"  {i}: {dict(row)}")
        
        return df
        
    except Exception as e:
        logger.error(f"ÐžÑˆÐ¸Ð±ÐºÐ° Ð·Ð°Ð³Ñ€ÑƒÐ·ÐºÐ¸ Ñ„Ð°Ð¹Ð»Ð°: {e}")
        return None


def extract_etf_tickers_from_registry(df):
    """Ð˜Ð·Ð²Ð»ÐµÑ‡ÐµÐ½Ð¸Ðµ Ñ‚Ð¸ÐºÐµÑ€Ð¾Ð² Ð‘ÐŸÐ˜Ð¤Ð¾Ð² Ð¸Ð· Ñ€ÐµÐµÑÑ‚Ñ€Ð°"""
    logger.info("Ð˜Ð·Ð²Ð»ÐµÑ‡ÐµÐ½Ð¸Ðµ Ñ‚Ð¸ÐºÐµÑ€Ð¾Ð² Ð‘ÐŸÐ˜Ð¤Ð¾Ð² Ð¸Ð· Ñ€ÐµÐµÑÑ‚Ñ€Ð°...")
    
    if df is None:
        return []
    
    # Ð˜Ñ‰ÐµÐ¼ ÐºÐ¾Ð»Ð¾Ð½ÐºÐ¸, ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ðµ Ð¼Ð¾Ð³ÑƒÑ‚ ÑÐ¾Ð´ÐµÑ€Ð¶Ð°Ñ‚ÑŒ Ñ‚Ð¸ÐºÐµÑ€Ñ‹ Ð¸Ð»Ð¸ ÐºÐ¾Ð´Ñ‹
    potential_ticker_columns = []
    for col in df.columns:
        col_lower = str(col).lower()
        if any(keyword in col_lower for keyword in ['ÐºÐ¾Ð´', 'Ñ‚Ð¸ÐºÐµÑ€', 'ticker', 'symbol', 'secid', 'isin']):
            potential_ticker_columns.append(col)
    
    logger.info(f"ÐŸÐ¾Ñ‚ÐµÐ½Ñ†Ð¸Ð°Ð»ÑŒÐ½Ñ‹Ðµ ÐºÐ¾Ð»Ð¾Ð½ÐºÐ¸ Ñ Ñ‚Ð¸ÐºÐµÑ€Ð°Ð¼Ð¸: {potential_ticker_columns}")
    
    # Ð˜Ñ‰ÐµÐ¼ ÐºÐ¾Ð»Ð¾Ð½ÐºÐ¸ Ñ Ñ‚Ð¸Ð¿Ð¾Ð¼ Ñ„Ð¾Ð½Ð´Ð°
    type_columns = []
    for col in df.columns:
        col_lower = str(col).lower()
        if any(keyword in col_lower for keyword in ['Ñ‚Ð¸Ð¿', 'type', 'ÐºÐ°Ñ‚ÐµÐ³Ð¾Ñ€Ð¸Ñ', 'Ð²Ð¸Ð´']):
            type_columns.append(col)
    
    logger.info(f"ÐšÐ¾Ð»Ð¾Ð½ÐºÐ¸ Ñ Ñ‚Ð¸Ð¿Ð¾Ð¼ Ñ„Ð¾Ð½Ð´Ð°: {type_columns}")
    
    # Ð¤Ð¸Ð»ÑŒÑ‚Ñ€ÑƒÐµÐ¼ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð‘ÐŸÐ˜Ð¤Ñ‹ (Ð±Ð¸Ñ€Ð¶ÐµÐ²Ñ‹Ðµ ÐŸÐ˜Ð¤Ñ‹)
    etf_df = df.copy()
    
    # Ð•ÑÐ»Ð¸ ÐµÑÑ‚ÑŒ ÐºÐ¾Ð»Ð¾Ð½ÐºÐ° Ñ Ñ‚Ð¸Ð¿Ð¾Ð¼, Ñ„Ð¸Ð»ÑŒÑ‚Ñ€ÑƒÐµÐ¼ Ð¿Ð¾ Ð‘ÐŸÐ˜Ð¤Ð°Ð¼
    if type_columns:
        type_col = type_columns[0]
        logger.info(f"Ð£Ð½Ð¸ÐºÐ°Ð»ÑŒÐ½Ñ‹Ðµ Ñ‚Ð¸Ð¿Ñ‹ Ñ„Ð¾Ð½Ð´Ð¾Ð² Ð² ÐºÐ¾Ð»Ð¾Ð½ÐºÐµ '{type_col}':")
        unique_types = df[type_col].value_counts()
        for type_name, count in unique_types.items():
            logger.info(f"  {type_name}: {count}")
        
        # Ð¤Ð¸Ð»ÑŒÑ‚Ñ€ÑƒÐµÐ¼ Ð‘ÐŸÐ˜Ð¤Ñ‹
        etf_keywords = ['Ð±Ð¸Ñ€Ð¶ÐµÐ²Ð¾Ð¹', 'Ð±Ð¿Ð¸Ñ„', 'etf', 'exchange', 'traded']
        etf_mask = df[type_col].astype(str).str.lower().str.contains('|'.join(etf_keywords), na=False)
        etf_df = df[etf_mask]
        logger.info(f"ÐÐ°Ð¹Ð´ÐµÐ½Ð¾ Ð‘ÐŸÐ˜Ð¤Ð¾Ð² Ð¿Ð¾ÑÐ»Ðµ Ñ„Ð¸Ð»ÑŒÑ‚Ñ€Ð°Ñ†Ð¸Ð¸: {len(etf_df)}")
    
    # Ð˜Ð·Ð²Ð»ÐµÐºÐ°ÐµÐ¼ Ñ‚Ð¸ÐºÐµÑ€Ñ‹
    tickers = []
    if potential_ticker_columns:
        ticker_col = potential_ticker_columns[0]
        tickers = etf_df[ticker_col].dropna().astype(str).tolist()
        # ÐžÑ‡Ð¸Ñ‰Ð°ÐµÐ¼ Ñ‚Ð¸ÐºÐµÑ€Ñ‹ Ð¾Ñ‚ Ð»Ð¸ÑˆÐ½Ð¸Ñ… ÑÐ¸Ð¼Ð²Ð¾Ð»Ð¾Ð²
        tickers = [ticker.strip().upper() for ticker in tickers if ticker.strip()]
        # Ð£Ð±Ð¸Ñ€Ð°ÐµÐ¼ Ð´ÑƒÐ±Ð»Ð¸ÐºÐ°Ñ‚Ñ‹
        tickers = list(set(tickers))
    
    logger.info(f"Ð˜Ð·Ð²Ð»ÐµÑ‡ÐµÐ½Ð¾ ÑƒÐ½Ð¸ÐºÐ°Ð»ÑŒÐ½Ñ‹Ñ… Ñ‚Ð¸ÐºÐµÑ€Ð¾Ð²: {len(tickers)}")
    logger.info(f"ÐŸÑ€Ð¸Ð¼ÐµÑ€Ñ‹ Ñ‚Ð¸ÐºÐµÑ€Ð¾Ð²: {tickers[:10]}")
    
    return tickers, etf_df


def get_moex_etf_list():
    """ÐŸÐ¾Ð»ÑƒÑ‡ÐµÐ½Ð¸Ðµ ÑÐ¿Ð¸ÑÐºÐ° Ð‘ÐŸÐ˜Ð¤Ð¾Ð² Ð½Ð°Ð¿Ñ€ÑÐ¼ÑƒÑŽ Ñ MOEX"""
    logger.info("ÐŸÐ¾Ð»ÑƒÑ‡ÐµÐ½Ð¸Ðµ ÑÐ¿Ð¸ÑÐºÐ° Ð‘ÐŸÐ˜Ð¤Ð¾Ð² Ñ MOEX...")
    
    try:
        collector = ETFDataCollectorWithFallback()
        etf_list = collector.get_etf_list()
        
        # Ð˜Ð·Ð²Ð»ÐµÐºÐ°ÐµÐ¼ Ñ‚Ð¸ÐºÐµÑ€Ñ‹ Ð¸Ð· ÑÐ¿Ð¸ÑÐºÐ°
        moex_tickers = []
        if etf_list:
            for etf in etf_list:
                if etf.get('ticker'):
                    moex_tickers.append(etf['ticker'])
        
        logger.info(f"ÐŸÐ¾Ð»ÑƒÑ‡ÐµÐ½Ð¾ Ñ‚Ð¸ÐºÐµÑ€Ð¾Ð² Ñ MOEX: {len(moex_tickers)}")
        logger.info(f"MOEX Ñ‚Ð¸ÐºÐµÑ€Ñ‹: {moex_tickers}")
        
        return moex_tickers
        
    except Exception as e:
        logger.error(f"ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿Ð¾Ð»ÑƒÑ‡ÐµÐ½Ð¸Ñ ÑÐ¿Ð¸ÑÐºÐ° Ñ MOEX: {e}")
        return []


def get_all_russian_etf_tickers():
    """ÐŸÐ¾Ð»ÑƒÑ‡ÐµÐ½Ð¸Ðµ Ð¿Ð¾Ð»Ð½Ð¾Ð³Ð¾ ÑÐ¿Ð¸ÑÐºÐ° Ñ€Ð¾ÑÑÐ¸Ð¹ÑÐºÐ¸Ñ… Ð‘ÐŸÐ˜Ð¤Ð¾Ð² Ð¸Ð· Ð²ÑÐµÑ… Ð¸ÑÑ‚Ð¾Ñ‡Ð½Ð¸ÐºÐ¾Ð²"""
    logger.info("Ð¡Ð±Ð¾Ñ€ Ð¿Ð¾Ð»Ð½Ð¾Ð³Ð¾ ÑÐ¿Ð¸ÑÐºÐ° Ñ€Ð¾ÑÑÐ¸Ð¹ÑÐºÐ¸Ñ… Ð‘ÐŸÐ˜Ð¤Ð¾Ð²...")
    
    all_tickers = set()
    
    # 1. Ð—Ð°Ð³Ñ€ÑƒÐ¶Ð°ÐµÐ¼ Ð¸Ð· Ñ€ÐµÐµÑÑ‚Ñ€Ð° Ð¦Ð‘
    cbr_df = load_cbr_pif_registry()
    if cbr_df is not None:
        cbr_tickers, etf_df = extract_etf_tickers_from_registry(cbr_df)
        all_tickers.update(cbr_tickers)
        logger.info(f"Ð˜Ð· Ñ€ÐµÐµÑÑ‚Ñ€Ð° Ð¦Ð‘ Ð´Ð¾Ð±Ð°Ð²Ð»ÐµÐ½Ð¾: {len(cbr_tickers)} Ñ‚Ð¸ÐºÐµÑ€Ð¾Ð²")
    
    # 2. ÐŸÐ¾Ð»ÑƒÑ‡Ð°ÐµÐ¼ Ñ MOEX
    moex_tickers = get_moex_etf_list()
    all_tickers.update(moex_tickers)
    logger.info(f"Ð¡ MOEX Ð´Ð¾Ð±Ð°Ð²Ð»ÐµÐ½Ð¾: {len(moex_tickers)} Ñ‚Ð¸ÐºÐµÑ€Ð¾Ð²")
    
    # 3. Ð”Ð¾Ð±Ð°Ð²Ð»ÑÐµÐ¼ Ð¸Ð·Ð²ÐµÑÑ‚Ð½Ñ‹Ðµ Ñ‚Ð¸ÐºÐµÑ€Ñ‹ Ð¸Ð· Ð½Ð°ÑˆÐµÐ¹ Ð±Ð°Ð·Ñ‹
    known_tickers = [
        'SBMX', 'VTBX', 'TECH', 'TGLD', 'FXRU', 'FXUS', 'FXGD', 'DIVD', 'SBGB', 'SBCB',
        'FXRB', 'FXRL', 'FXMM', 'FXTP', 'FXCN', 'FXDE', 'FXIT', 'FXKZ', 'FXWO', 'FXRW',
        'TMOS', 'TIPO', 'TBIO', 'TEUR', 'TUSD', 'TCNY', 'TSPX', 'TNPG', 'TRUR', 'TGRU',
        'SBSP', 'SBMM', 'SBGD', 'SBCL', 'SBAG', 'SBPD', 'SBCS', 'SBHI', 'SBEM', 'SBEU',
        'VTBA', 'VTBB', 'VTBE', 'VTBG', 'VTBM', 'VTBN', 'VTBR', 'VTBS', 'VTBU', 'VTBZ',
        'AKSP', 'AKMM', 'AKGD', 'AKCL', 'AKAG', 'AKPD', 'AKCS', 'AKHI', 'AKEM', 'AKEU'
    ]
    all_tickers.update(known_tickers)
    logger.info(f"Ð˜Ð· Ð±Ð°Ð·Ñ‹ Ð·Ð½Ð°Ð½Ð¸Ð¹ Ð´Ð¾Ð±Ð°Ð²Ð»ÐµÐ½Ð¾: {len(known_tickers)} Ñ‚Ð¸ÐºÐµÑ€Ð¾Ð²")
    
    # ÐšÐ¾Ð½Ð²ÐµÑ€Ñ‚Ð¸Ñ€ÑƒÐµÐ¼ Ð² Ð¾Ñ‚ÑÐ¾Ñ€Ñ‚Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ñ‹Ð¹ ÑÐ¿Ð¸ÑÐ¾Ðº
    final_tickers = sorted(list(all_tickers))
    
    logger.info(f"Ð˜Ð¢ÐžÐ“Ðž ÑƒÐ½Ð¸ÐºÐ°Ð»ÑŒÐ½Ñ‹Ñ… Ñ‚Ð¸ÐºÐµÑ€Ð¾Ð² Ð´Ð»Ñ Ð°Ð½Ð°Ð»Ð¸Ð·Ð°: {len(final_tickers)}")
    
    return final_tickers


def analyze_all_russian_etfs():
    """ÐÐ½Ð°Ð»Ð¸Ð· Ð²ÑÐµÑ… Ñ€Ð¾ÑÑÐ¸Ð¹ÑÐºÐ¸Ñ… Ð‘ÐŸÐ˜Ð¤Ð¾Ð²"""
    logger.info("=== Ð—ÐÐŸÐ£Ð¡Ðš ÐŸÐžÐ›ÐÐžÐ“Ðž ÐÐÐÐ›Ð˜Ð—Ð Ð’Ð¡Ð•Ð¥ Ð ÐžÐ¡Ð¡Ð˜Ð™Ð¡ÐšÐ˜Ð¥ Ð‘ÐŸÐ˜Ð¤ ===")
    
    # ÐŸÐ¾Ð»ÑƒÑ‡Ð°ÐµÐ¼ Ð¿Ð¾Ð»Ð½Ñ‹Ð¹ ÑÐ¿Ð¸ÑÐ¾Ðº Ñ‚Ð¸ÐºÐµÑ€Ð¾Ð²
    all_tickers = get_all_russian_etf_tickers()
    
    if not all_tickers:
        logger.error("ÐÐµ ÑƒÐ´Ð°Ð»Ð¾ÑÑŒ Ð¿Ð¾Ð»ÑƒÑ‡Ð¸Ñ‚ÑŒ ÑÐ¿Ð¸ÑÐ¾Ðº Ñ‚Ð¸ÐºÐµÑ€Ð¾Ð² Ð´Ð»Ñ Ð°Ð½Ð°Ð»Ð¸Ð·Ð°")
        return
    
    logger.info(f"ÐÐ°Ñ‡Ð¸Ð½Ð°ÐµÐ¼ Ð°Ð½Ð°Ð»Ð¸Ð· {len(all_tickers)} Ð‘ÐŸÐ˜Ð¤Ð¾Ð²...")
    
    # Ð¡Ð¾Ð·Ð´Ð°ÐµÐ¼ ÐºÐ¾Ð»Ð»ÐµÐºÑ‚Ð¾Ñ€
    collector = ETFDataCollectorWithFallback()
    
    # Ð ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ñ‹ Ð°Ð½Ð°Ð»Ð¸Ð·Ð°
    successful_data = []
    failed_tickers = []
    
    # ÐÐ½Ð°Ð»Ð¸Ð·Ð¸Ñ€ÑƒÐµÐ¼ ÐºÐ°Ð¶Ð´Ñ‹Ð¹ Ñ‚Ð¸ÐºÐµÑ€
    for i, ticker in enumerate(all_tickers, 1):
        logger.info(f"[{i}/{len(all_tickers)}] ÐÐ½Ð°Ð»Ð¸Ð· {ticker}...")
        
        try:
            etf_data = collector.collect_etf_data(ticker)
            
            if etf_data and etf_data.get('current_price') is not None:
                successful_data.append(etf_data)
                logger.info(f"  âœ“ {ticker}: Ñ†ÐµÐ½Ð° {etf_data.get('current_price')} Ñ€ÑƒÐ±, Ð¸ÑÑ‚Ð¾Ñ‡Ð½Ð¸Ðº: {etf_data.get('data_source')}")
            else:
                failed_tickers.append(ticker)
                logger.warning(f"  âœ— {ticker}: Ð´Ð°Ð½Ð½Ñ‹Ðµ Ð½ÐµÐ´Ð¾ÑÑ‚ÑƒÐ¿Ð½Ñ‹")
                
        except Exception as e:
            failed_tickers.append(ticker)
            logger.error(f"  âœ— {ticker}: Ð¾ÑˆÐ¸Ð±ÐºÐ° - {e}")
        
        # ÐŸÐ°ÑƒÐ·Ð° Ð¼ÐµÐ¶Ð´Ñƒ Ð·Ð°Ð¿Ñ€Ð¾ÑÐ°Ð¼Ð¸
        time.sleep(0.3)
        
        # ÐŸÑ€Ð¾Ð¼ÐµÐ¶ÑƒÑ‚Ð¾Ñ‡Ð½Ð°Ñ ÑÑ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÐ° ÐºÐ°Ð¶Ð´Ñ‹Ðµ 10 Ñ‚Ð¸ÐºÐµÑ€Ð¾Ð²
        if i % 10 == 0:
            logger.info(f"ÐŸÑ€Ð¾Ð¼ÐµÐ¶ÑƒÑ‚Ð¾Ñ‡Ð½Ð°Ñ ÑÑ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÐ°: ÑƒÑÐ¿ÐµÑˆÐ½Ð¾ {len(successful_data)}, Ð½ÐµÑƒÐ´Ð°Ñ‡Ð½Ð¾ {len(failed_tickers)}")
    
    # Ð¡Ð¾Ð·Ð´Ð°ÐµÐ¼ DataFrame Ñ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ð°Ð¼Ð¸
    if successful_data:
        df = pd.DataFrame(successful_data)
        
        # Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÑÐµÐ¼ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ñ‹
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # CSV Ñ„Ð°Ð¹Ð»
        csv_file = f'full_russian_etf_analysis_{timestamp}.csv'
        df.to_csv(csv_file, index=False, encoding='utf-8')
        
        # JSON Ð¾Ñ‚Ñ‡ÐµÑ‚
        report = {
            'analysis_date': datetime.now().isoformat(),
            'total_tickers_analyzed': len(all_tickers),
            'successful_analysis': len(successful_data),
            'failed_analysis': len(failed_tickers),
            'success_rate': len(successful_data) / len(all_tickers) * 100,
            'failed_tickers': failed_tickers,
            'summary_statistics': {
                'total_etfs': len(df),
                'average_price': df['current_price'].mean() if 'current_price' in df.columns else None,
                'price_range': {
                    'min': df['current_price'].min() if 'current_price' in df.columns else None,
                    'max': df['current_price'].max() if 'current_price' in df.columns else None
                },
                'data_sources': df['data_source'].value_counts().to_dict() if 'data_source' in df.columns else {},
                'management_companies': df['management_company'].value_counts().to_dict() if 'management_company' in df.columns else {}
            }
        }
        
        json_file = f'full_russian_etf_report_{timestamp}.json'
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2, default=str)
        
        # Ð’Ñ‹Ð²Ð¾Ð´Ð¸Ð¼ Ð¸Ñ‚Ð¾Ð³Ð¾Ð²ÑƒÑŽ ÑÑ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÑƒ
        print_final_statistics(report, df)
        
        logger.info(f"ÐÐ½Ð°Ð»Ð¸Ð· Ð·Ð°Ð²ÐµÑ€ÑˆÐµÐ½!")
        logger.info(f"  ðŸ“Š Ð”Ð°Ð½Ð½Ñ‹Ðµ: {csv_file}")
        logger.info(f"  ðŸ“‹ ÐžÑ‚Ñ‡ÐµÑ‚: {json_file}")
        
        return df, report
    
    else:
        logger.error("ÐÐµ ÑƒÐ´Ð°Ð»Ð¾ÑÑŒ Ð¿Ð¾Ð»ÑƒÑ‡Ð¸Ñ‚ÑŒ Ð´Ð°Ð½Ð½Ñ‹Ðµ Ð½Ð¸ Ð¿Ð¾ Ð¾Ð´Ð½Ð¾Ð¼Ñƒ Ð‘ÐŸÐ˜Ð¤Ñƒ")
        return None, None


def print_final_statistics(report, df):
    """Ð’Ñ‹Ð²Ð¾Ð´ Ð¸Ñ‚Ð¾Ð³Ð¾Ð²Ð¾Ð¹ ÑÑ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÐ¸"""
    print("\n" + "="*80)
    print("ðŸ† Ð˜Ð¢ÐžÐ“ÐžÐ’ÐÐ¯ Ð¡Ð¢ÐÐ¢Ð˜Ð¡Ð¢Ð˜ÐšÐ ÐŸÐžÐ›ÐÐžÐ“Ðž ÐÐÐÐ›Ð˜Ð—Ð Ð ÐžÐ¡Ð¡Ð˜Ð™Ð¡ÐšÐ˜Ð¥ Ð‘ÐŸÐ˜Ð¤")
    print("="*80)
    
    print(f"\nðŸ“Š ÐžÐ‘Ð©ÐÐ¯ Ð˜ÐÐ¤ÐžÐ ÐœÐÐ¦Ð˜Ð¯:")
    print(f"   Ð’ÑÐµÐ³Ð¾ Ñ‚Ð¸ÐºÐµÑ€Ð¾Ð² Ð¿Ñ€Ð¾Ð°Ð½Ð°Ð»Ð¸Ð·Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¾: {report['total_tickers_analyzed']}")
    print(f"   Ð£ÑÐ¿ÐµÑˆÐ½Ð¾ Ð¿Ð¾Ð»ÑƒÑ‡ÐµÐ½Ñ‹ Ð´Ð°Ð½Ð½Ñ‹Ðµ: {report['successful_analysis']}")
    print(f"   ÐÐµÑƒÐ´Ð°Ñ‡Ð½Ñ‹Ñ… Ð¿Ð¾Ð¿Ñ‹Ñ‚Ð¾Ðº: {report['failed_analysis']}")
    print(f"   ÐŸÑ€Ð¾Ñ†ÐµÐ½Ñ‚ ÑƒÑÐ¿ÐµÑˆÐ½Ð¾ÑÑ‚Ð¸: {report['success_rate']:.1f}%")
    
    if 'summary_statistics' in report and report['summary_statistics']:
        stats = report['summary_statistics']
        
        if stats.get('average_price'):
            print(f"\nðŸ’° Ð¦Ð•ÐÐžÐ’ÐÐ¯ Ð¡Ð¢ÐÐ¢Ð˜Ð¡Ð¢Ð˜ÐšÐ:")
            print(f"   Ð¡Ñ€ÐµÐ´Ð½ÑÑ Ñ†ÐµÐ½Ð°: {stats['average_price']:.2f} Ñ€ÑƒÐ±")
            print(f"   Ð”Ð¸Ð°Ð¿Ð°Ð·Ð¾Ð½ Ñ†ÐµÐ½: {stats['price_range']['min']:.2f} - {stats['price_range']['max']:.2f} Ñ€ÑƒÐ±")
        
        if stats.get('data_sources'):
            print(f"\nðŸ”Œ Ð˜Ð¡Ð¢ÐžÐ§ÐÐ˜ÐšÐ˜ Ð”ÐÐÐÐ«Ð¥:")
            for source, count in stats['data_sources'].items():
                print(f"   {source}: {count} Ñ„Ð¾Ð½Ð´Ð¾Ð²")
        
        if stats.get('management_companies'):
            print(f"\nðŸ¢ Ð£ÐŸÐ ÐÐ’Ð›Ð¯Ð®Ð©Ð˜Ð• ÐšÐžÐœÐŸÐÐÐ˜Ð˜ (Ñ‚Ð¾Ð¿-10):")
            sorted_companies = sorted(stats['management_companies'].items(), key=lambda x: x[1], reverse=True)
            for company, count in sorted_companies[:10]:
                print(f"   {company}: {count} Ñ„Ð¾Ð½Ð´Ð¾Ð²")
    
    if report.get('failed_tickers'):
        print(f"\nâŒ ÐÐ•Ð£Ð”ÐÐ§ÐÐ«Ð• Ð¢Ð˜ÐšÐ•Ð Ð« (Ð¿ÐµÑ€Ð²Ñ‹Ðµ 20):")
        for ticker in report['failed_tickers'][:20]:
            print(f"   {ticker}")
        if len(report['failed_tickers']) > 20:
            print(f"   ... Ð¸ ÐµÑ‰Ðµ {len(report['failed_tickers']) - 20}")


if __name__ == "__main__":
    analyze_all_russian_etfs()